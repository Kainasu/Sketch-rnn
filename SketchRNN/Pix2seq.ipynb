{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CO1A6ZlGtN-"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/quickdraw_dataset/sketchrnn/cat.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4GvzWJkDDNT"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cairosvg"
      ],
      "metadata": {
        "id": "lH9-QGb_xT9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgwrite"
      ],
      "metadata": {
        "id": "ALJJldo3iYKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "gbzcD-m49_dP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import einops\n",
        "\n",
        "from PIL import Image\n",
        "import cairosvg\n",
        "import os\n",
        "from six.moves import range\n",
        "import svgwrite\n",
        "import io\n",
        "import tensorflow as tf\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bounds(data, factor=10):\n",
        "    \"\"\"Return bounds of data.\"\"\"\n",
        "    min_x = 0\n",
        "    max_x = 0\n",
        "    min_y = 0\n",
        "    max_y = 0\n",
        "\n",
        "    abs_x = 0\n",
        "    abs_y = 0\n",
        "    for i in range(len(data)):\n",
        "        x = float(data[i, 0]) / factor\n",
        "        y = float(data[i, 1]) / factor\n",
        "        abs_x += x\n",
        "        abs_y += y\n",
        "        min_x = min(min_x, abs_x)\n",
        "        min_y = min(min_y, abs_y)\n",
        "        max_x = max(max_x, abs_x)\n",
        "        max_y = max(max_y, abs_y)\n",
        "\n",
        "    return min_x, max_x, min_y, max_y"
      ],
      "metadata": {
        "id": "Zm6NASJyjUoa"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_len(strokes):\n",
        "    \"\"\"Return the maximum length of an array of strokes.\"\"\"\n",
        "    max_len = 0\n",
        "    for stroke in strokes:  # stroke: [N_points, 3]\n",
        "        ml = len(stroke)\n",
        "        max_len = ml if ml > max_len else max_len\n",
        "\n",
        "    return max_len"
      ],
      "metadata": {
        "id": "Idb68UU_dGXt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_strokes(data, svg_filename, factor=0.2, padding=50):\n",
        "    \"\"\"\n",
        "    little function that displays vector images and saves them to .svg\n",
        "    :param data:\n",
        "    :param factor:\n",
        "    :param svg_filename:\n",
        "    :param padding:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
        "    dims = (padding + max_x - min_x, padding + max_y - min_y)\n",
        "    dwg = svgwrite.Drawing(size=dims)\n",
        "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
        "    lift_pen = 1\n",
        "    abs_x = int(padding / 2) - min_x\n",
        "    abs_y = int(padding / 2) - min_y\n",
        "    p = \"M%s, %s \" % (abs_x, abs_y)\n",
        "    # use lowcase for relative position\n",
        "    command = \"m\"\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        if lift_pen == 1:\n",
        "            command = \"m\"\n",
        "        elif command != \"l\":\n",
        "            command = \"l\"\n",
        "        else:\n",
        "            command = \"\"\n",
        "        x = float(data[i, 0]) / factor\n",
        "        y = float(data[i, 1]) / factor\n",
        "        lift_pen = data[i, 2]\n",
        "        p += command + str(x) + \", \" + str(y) + \" \"\n",
        "    the_color = \"black\"\n",
        "    stroke_width = 1\n",
        "\n",
        "    dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(\"none\"))\n",
        "    #dwg.save()\n",
        "\n",
        "    svg_code = dwg.tostring()\n",
        "    img = cairosvg.svg2png(bytestring=svg_code)\n",
        "    image = Image.open(io.BytesIO(img))\n",
        "    image = image.resize((28,28))\n",
        "    #image = image.convert('1')\n",
        "    aarr = np.asarray(image)\n",
        "    # aarr = np.reshape(aarr, (28*28))\n",
        "    # np.save('array', aarr)\n",
        "    #image.save(svg_filename + '.png')\n",
        "    return dims, dwg.tostring(), aarr"
      ],
      "metadata": {
        "id": "kHOwTBiEiSjJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(data_dir, data_set):\n",
        "    \"\"\"Loads the .npz file, and splits the set into train/valid/test.\"\"\"\n",
        "\n",
        "    # normalizes the x and y columns using the training set.\n",
        "    # applies same scaling factor to valid and test set.\n",
        "    img_H, img_W = 48, 48\n",
        "\n",
        "    if isinstance(data_set, list):\n",
        "        datasets = data_set\n",
        "    else:\n",
        "        datasets = [data_set]\n",
        "\n",
        "    train_strokes = None\n",
        "    valid_strokes = None\n",
        "    test_strokes = None\n",
        "\n",
        "    png_paths_map = {'train': [], 'valid': [], 'test': []}\n",
        "\n",
        "    for dataset in datasets:\n",
        "        data_filepath = os.path.join(data_dir, 'npz', dataset)\n",
        "        data = np.load(data_filepath, encoding='latin1', allow_pickle=True)\n",
        "        print('Loaded {}/{}/{} from {}'.format(\n",
        "            len(data['train']), len(data['valid']), len(data['test']),\n",
        "            dataset))\n",
        "        if train_strokes is None:\n",
        "            train_strokes = data['train']  # [N (#sketches),], each with [S (#points), 3]\n",
        "            valid_strokes = data['valid']\n",
        "            test_strokes = data['test']\n",
        "        else:\n",
        "            train_strokes = np.concatenate((train_strokes, data['train']))\n",
        "            valid_strokes = np.concatenate((valid_strokes, data['valid']))\n",
        "            test_strokes = np.concatenate((test_strokes, data['test']))\n",
        "\n",
        "        splits = ['train', 'valid', 'test']\n",
        "        for split in splits:\n",
        "            for im_idx in range(len(data[split])):\n",
        "                png_path = os.path.join(data_dir, 'png', dataset[:-4], split,\n",
        "                                        str(img_H) + 'x' + str(img_W), str(im_idx) + '.png')\n",
        "                png_paths_map[split].append(png_path)\n",
        "\n",
        "    all_strokes = np.concatenate((train_strokes, valid_strokes, test_strokes))\n",
        "    num_points = 0\n",
        "    for stroke in all_strokes:\n",
        "        num_points += len(stroke)\n",
        "    avg_len = num_points / len(all_strokes)\n",
        "    print('Dataset combined: {} ({}/{}/{}), avg len {}'.format(\n",
        "        len(all_strokes), len(train_strokes), len(valid_strokes),\n",
        "        len(test_strokes), int(avg_len)))\n",
        "    assert len(train_strokes) == len(png_paths_map['train'])\n",
        "    assert len(valid_strokes) == len(png_paths_map['valid'])\n",
        "    assert len(test_strokes) == len(png_paths_map['test'])\n",
        "\n",
        "    # calculate the max strokes we need.\n",
        "    max_seq_len = get_max_len(all_strokes)\n",
        "\n",
        "    result = [train_strokes, valid_strokes, test_strokes]\n",
        "    png_paths = [png_paths_map['train'], png_paths_map['valid'], png_paths_map['test'] ]\n",
        "    return result, png_paths"
      ],
      "metadata": {
        "id": "vg7nT39JamgW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_image(png_filename, pngsize):\n",
        "    curr_png = Image.open(png_filename).convert('RGB')\n",
        "    png_curr_w = curr_png.width\n",
        "    png_curr_h = curr_png.height\n",
        "    print(\"pngsize: {}, {}\".format(pngsize[0],pngsize[1]))\n",
        "\n",
        "    if png_curr_w != pngsize[0] and png_curr_h != pngsize[1]:\n",
        "        print('Not aligned', 'png_curr_w', png_curr_w, 'png_curr_h', png_curr_h)\n",
        "\n",
        "    padded_png = np.zeros(shape=[pngsize[1], pngsize[0], 3], dtype=np.uint8)\n",
        "    padded_png.fill(255)\n",
        "\n",
        "    if png_curr_w > png_curr_h:\n",
        "        pad = int(round((png_curr_w - png_curr_h) / 2))\n",
        "        padded_png[pad: pad + png_curr_h, :png_curr_w, :] = np.array(curr_png, dtype=np.uint8)\n",
        "    else:\n",
        "        pad = int(round((png_curr_h - png_curr_w) / 2))\n",
        "        padded_png[:png_curr_h, pad: pad + png_curr_w, :] = np.array(curr_png, dtype=np.uint8)\n",
        "\n",
        "    padded_png = Image.fromarray(padded_png, 'RGB')\n",
        "    padded_png.save(png_filename, 'PNG')"
      ],
      "metadata": {
        "id": "0Nm2TnJ5w9nD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svg2png(dwg_string, svgsize, pngsize, png_filename, padding=False):\n",
        "    \"\"\"convert svg into png, using cairosvg\"\"\"\n",
        "    svg_w, svg_h = svgsize\n",
        "    png_w, png_h = pngsize\n",
        "    x_scale = png_w / svg_w\n",
        "    y_scale = png_h / svg_h\n",
        "\n",
        "    if x_scale > y_scale:\n",
        "        cairosvg.svg2png(bytestring=dwg_string, write_to=png_filename, output_height=png_h)\n",
        "    else:\n",
        "        cairosvg.svg2png(bytestring=dwg_string, write_to=png_filename, output_width=png_w)\n",
        "\n",
        "    if padding:\n",
        "        pad_image(png_filename, pngsize)\n",
        "    img = Image.open(png_filename)\n",
        "    img = img.resize((48,48))\n",
        "    img.save(png_filename, 'PNG')"
      ],
      "metadata": {
        "id": "ABNDnquuxL4Z"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_svg2bitmap(data_base_dir, data_set):\n",
        "    img_H, img_W = 250, 250\n",
        "\n",
        "    npz_dir = os.path.join(data_base_dir, 'npz')\n",
        "    svg_dir = os.path.join(data_base_dir, 'svg')\n",
        "    png_dir = os.path.join(data_base_dir, 'png')\n",
        "\n",
        "    #model_params = sketch_rnn_model.get_default_hparams()\n",
        "    \n",
        "    for dataset_i in range(len(data_set)):\n",
        "        assert data_set[dataset_i][-4:] == '.npz'\n",
        "        cate_svg_dir = os.path.join(svg_dir, data_set[dataset_i][:-4])\n",
        "        cate_png_dir = os.path.join(png_dir, data_set[dataset_i][:-4])\n",
        "\n",
        "        datasets, png_paths = load_dataset(data_base_dir, data_set)\n",
        "\n",
        "        data_types = ['train', 'valid', 'test']\n",
        "        for d_i, data_type in enumerate(data_types):\n",
        "            split_cate_svg_dir = os.path.join(cate_svg_dir, data_type)\n",
        "            split_cate_png_dir = os.path.join(cate_png_dir, data_type,\n",
        "                                              str(48) + 'x' + str(48))\n",
        "\n",
        "            os.makedirs(split_cate_svg_dir, exist_ok=True)\n",
        "            os.makedirs(split_cate_png_dir, exist_ok=True)\n",
        "\n",
        "            split_dataset = datasets[d_i]\n",
        "\n",
        "            for ex_idx in range(len(split_dataset)):\n",
        "                stroke = np.copy(split_dataset[ex_idx])\n",
        "                print('example_idx', ex_idx, 'stroke.shape', stroke.shape)\n",
        "\n",
        "                png_path = png_paths[d_i][ex_idx]\n",
        "                # print(\"HOHOHO\")\n",
        "                # print(split_cate_png_dir)\n",
        "                # print(png_path[:len(split_cate_png_dir)])\n",
        "                # assert split_cate_png_dir == png_path[:len(split_cate_png_dir)]\n",
        "                actual_idx = png_path[len(split_cate_png_dir) + 1:-4]\n",
        "                svg_path = os.path.join(split_cate_svg_dir, str(actual_idx) + '.svg')\n",
        "\n",
        "                svg_size, dwg_bytestring, aarr = draw_strokes(stroke, svg_path, padding=10)  # (w, h)\n",
        "                svg2png(dwg_bytestring, svg_size, (img_W, img_H), png_path,\n",
        "                               padding=True)\n",
        "                \n",
        "#render_svg2bitmap('./', ['cat.npz'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4bqaodMly5oz",
        "outputId": "d06dbace-b2b6-4306-9aa7-dfbd913af2f5"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 70000/2500/2500 from cat.npz\n",
            "Dataset combined: 75000 (70000/2500/2500), avg len 69\n",
            "example_idx 0 stroke.shape (96, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 1 stroke.shape (81, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 2 stroke.shape (76, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 3 stroke.shape (67, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 4 stroke.shape (47, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 5 stroke.shape (102, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 6 stroke.shape (81, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 7 stroke.shape (38, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 8 stroke.shape (60, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 9 stroke.shape (126, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 10 stroke.shape (92, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 11 stroke.shape (75, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 12 stroke.shape (71, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 13 stroke.shape (105, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 14 stroke.shape (84, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 15 stroke.shape (73, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 16 stroke.shape (68, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 17 stroke.shape (69, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 18 stroke.shape (60, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 19 stroke.shape (83, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 20 stroke.shape (61, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 21 stroke.shape (56, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 22 stroke.shape (51, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 23 stroke.shape (69, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 24 stroke.shape (89, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 25 stroke.shape (48, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 26 stroke.shape (69, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 27 stroke.shape (118, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 28 stroke.shape (93, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 29 stroke.shape (69, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 30 stroke.shape (50, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 31 stroke.shape (43, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 32 stroke.shape (61, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 33 stroke.shape (56, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 34 stroke.shape (46, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 35 stroke.shape (55, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 36 stroke.shape (92, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 37 stroke.shape (78, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 38 stroke.shape (128, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 39 stroke.shape (51, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 40 stroke.shape (59, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 41 stroke.shape (91, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 42 stroke.shape (73, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 43 stroke.shape (39, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 44 stroke.shape (43, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 45 stroke.shape (82, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 46 stroke.shape (104, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 47 stroke.shape (105, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 48 stroke.shape (66, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 49 stroke.shape (90, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 50 stroke.shape (54, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 51 stroke.shape (41, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 52 stroke.shape (72, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 53 stroke.shape (60, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 54 stroke.shape (61, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 55 stroke.shape (91, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 56 stroke.shape (79, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 57 stroke.shape (52, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 58 stroke.shape (40, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 59 stroke.shape (75, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 60 stroke.shape (57, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 61 stroke.shape (46, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 62 stroke.shape (58, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 63 stroke.shape (72, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 64 stroke.shape (81, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 65 stroke.shape (76, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 66 stroke.shape (94, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 67 stroke.shape (91, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 68 stroke.shape (85, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 69 stroke.shape (107, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 70 stroke.shape (51, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 71 stroke.shape (113, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 72 stroke.shape (44, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 73 stroke.shape (46, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 74 stroke.shape (51, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 75 stroke.shape (75, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 76 stroke.shape (80, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 77 stroke.shape (52, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 78 stroke.shape (63, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 79 stroke.shape (124, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 80 stroke.shape (67, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 81 stroke.shape (66, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 82 stroke.shape (84, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 83 stroke.shape (114, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 84 stroke.shape (86, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 85 stroke.shape (62, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 86 stroke.shape (53, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 87 stroke.shape (59, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 88 stroke.shape (71, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 89 stroke.shape (46, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 90 stroke.shape (78, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 91 stroke.shape (46, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 92 stroke.shape (53, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 93 stroke.shape (79, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 94 stroke.shape (66, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 95 stroke.shape (42, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 96 stroke.shape (103, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 97 stroke.shape (104, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 98 stroke.shape (57, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 99 stroke.shape (69, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 100 stroke.shape (97, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 101 stroke.shape (65, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 102 stroke.shape (108, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 103 stroke.shape (95, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 104 stroke.shape (39, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 105 stroke.shape (44, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 106 stroke.shape (82, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 107 stroke.shape (58, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 108 stroke.shape (46, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 109 stroke.shape (75, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 110 stroke.shape (73, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 111 stroke.shape (46, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 112 stroke.shape (62, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 113 stroke.shape (41, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 114 stroke.shape (54, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 115 stroke.shape (53, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 116 stroke.shape (61, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 117 stroke.shape (55, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 118 stroke.shape (41, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 119 stroke.shape (121, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 120 stroke.shape (124, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 121 stroke.shape (78, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 122 stroke.shape (60, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 123 stroke.shape (58, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 124 stroke.shape (63, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 125 stroke.shape (103, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 126 stroke.shape (77, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 127 stroke.shape (75, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 128 stroke.shape (69, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 129 stroke.shape (63, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 130 stroke.shape (79, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 131 stroke.shape (71, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 132 stroke.shape (67, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 133 stroke.shape (69, 3)\n",
            "pngsize: 250, 250\n",
            "example_idx 134 stroke.shape (42, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored from cffi callback <function _make_write_func.<locals>.write_func at 0x7f76c5d85f70>:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/cairocffi/surfaces.py\", line 45, in write_func\n",
            "    @ffi.callback(\"cairo_write_func_t\", error=constants.STATUS_WRITE_ERROR)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pngsize: 250, 250\n",
            "example_idx 135 stroke.shape (63, 3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-eeb9c13158d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                padding=True)\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mrender_svg2bitmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cat.npz'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-146-eeb9c13158d7>\u001b[0m in \u001b[0;36mrender_svg2bitmap\u001b[0;34m(data_base_dir, data_set)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0msvg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_cate_svg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0msvg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdwg_bytestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_strokes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (w, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 svg2png(dwg_bytestring, svg_size, (img_W, img_H), png_path,\n\u001b[1;32m     42\u001b[0m                                padding=True)\n",
            "\u001b[0;32m<ipython-input-53-fa685ccb72ae>\u001b[0m in \u001b[0;36mdraw_strokes\u001b[0;34m(data, svg_filename, factor, padding)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0msvg_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdwg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcairosvg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvg2png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytestring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvg_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/cairosvg/__init__.py\u001b[0m in \u001b[0;36msvg2png\u001b[0;34m(bytestring, file_obj, url, dpi, parent_width, parent_height, scale, unsafe, background_color, negate_colors, invert_images, write_to, output_width, output_height)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegate_colors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             write_to=None, output_width=None, output_height=None):\n\u001b[0;32m---> 55\u001b[0;31m     return surface.PNGSurface.convert(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mbytestring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbytestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mparent_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/cairosvg/surface.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(cls, bytestring, file_obj, url, dpi, parent_width, parent_height, scale, unsafe, background_color, negate_colors, invert_images, write_to, output_width, output_height, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mmap_rgba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegate_color\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnegate_colors\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             map_image=invert_image if invert_images else None)\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrite_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/cairosvg/surface.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;34m\"\"\"Read the PNG surface content.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcairo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_to_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/cairocffi/surfaces.py\u001b[0m in \u001b[0;36mwrite_to_png\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0mwrite_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_write_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m                 _check_status(cairo.cairo_surface_write_to_png_stream(\n\u001b[0m\u001b[1;32m    661\u001b[0m                     self._pointer, write_func, ffi.NULL))\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/cairocffi/__init__.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m     86\u001b[0m         message = 'cairo returned %s: %s' % (\n\u001b[1;32m     87\u001b[0m             status_name, ffi.string(cairo.cairo_status_to_string(status)))\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno cairo returned CAIRO_STATUS_WRITE_ERROR: b'error while writing to output stream'] 11"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "datasets, png_paths = load_dataset('./', ['cat.npz'])\n",
        "stroke = np.copy(datasets[0][0])\n",
        "svg_size, dwg_bytestring, aarr = draw_strokes(stroke, './', padding=10)\n",
        "#cairosvg.svg2png(bytestring=dwg_bytestring, write_to='./cat.png')\n",
        "\n",
        "svg2png(dwg_bytestring, svg_size, (250, 250), './cat.png',\n",
        "                               padding=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqIY89YgkNRS",
        "outputId": "dae1cff1-1234-4886-f50e-aa7fb9794f5a"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 70000/2500/2500 from cat.npz\n",
            "Dataset combined: 75000 (70000/2500/2500), avg len 69\n",
            "pngsize: 110, 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSgcOTcP-Oag"
      },
      "outputs": [],
      "source": [
        "class StrokesDatasetTest(Dataset):\n",
        "    \"\"\"\n",
        "    ## Dataset\n",
        "    This class loads and pre-processes the data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset: np.array, max_seq_length: int, scale: Optional[float] = None):\n",
        "        \"\"\"\n",
        "        `dataset` is a list of numpy arrays of shape [seq_len, 3].\n",
        "        It is a sequence of strokes, and each stroke is represented by\n",
        "        3 integers.\n",
        "        First two are the displacements along x and y ($\\Delta x$, $\\Delta y$)\n",
        "        and the last integer represents the state of the pen, $1$ if it's touching\n",
        "        the paper and $0$ otherwise.\n",
        "        \"\"\"\n",
        "\n",
        "        data = []\n",
        "        # We iterate through each of the sequences and filter\n",
        "        for seq in dataset:\n",
        "            # Filter if the length of the sequence of strokes is within our range\n",
        "            if 10 < len(seq) <= max_seq_length:\n",
        "                # Clamp $\\Delta x$, $\\Delta y$ to $[-1000, 1000]$\n",
        "                seq = np.minimum(seq, 1000)\n",
        "                seq = np.maximum(seq, -1000)\n",
        "                # Convert to a floating point array and add to `data`\n",
        "                seq = np.array(seq, dtype=np.float32)\n",
        "                data.append(seq)\n",
        "\n",
        "        # We then calculate the scaling factor which is the\n",
        "        # standard deviation of ($\\Delta x$, $\\Delta y$) combined.\n",
        "        # Paper notes that the mean is not adjusted for simplicity,\n",
        "        # since the mean is anyway close to $0$.\n",
        "        if scale is None:\n",
        "            scale = np.std(np.concatenate([np.ravel(s[:, 0:2]) for s in data]))\n",
        "        self.scale = scale\n",
        "\n",
        "        # Get the longest sequence length among all sequences\n",
        "        longest_seq_len = max([len(seq) for seq in data])\n",
        "\n",
        "        # We initialize PyTorch data array with two extra steps for start-of-sequence (sos)\n",
        "        # and end-of-sequence (eos).\n",
        "        # Each step is a vector $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
        "        # Only one of $p_1, p_2, p_3$ is $1$ and the others are $0$.\n",
        "        # They represent *pen down*, *pen up* and *end-of-sequence* in that order.\n",
        "        # $p_1$ is $1$ if the pen touches the paper in the next step.\n",
        "        # $p_2$ is $1$ if the pen doesn't touch the paper in the next step.\n",
        "        # $p_3$ is $1$ if it is the end of the drawing.\n",
        "        self.data = torch.zeros(len(data), longest_seq_len + 2, 5, dtype=torch.float)\n",
        "        # The mask array needs only one extra-step since it is for the outputs of the\n",
        "        # decoder, which takes in `data[:-1]` and predicts next step.\n",
        "        self.mask = torch.zeros(len(data), longest_seq_len + 1)\n",
        "\n",
        "        for i, seq in enumerate(data):\n",
        "            seq = torch.from_numpy(seq)\n",
        "            len_seq = len(seq)\n",
        "            # Scale and set $\\Delta x, \\Delta y$\n",
        "            self.data[i, 1:len_seq + 1, :2] = seq[:, :2] / scale\n",
        "            # $p_1$\n",
        "            self.data[i, 1:len_seq + 1, 2] = 1 - seq[:, 2]\n",
        "            # $p_2$\n",
        "            self.data[i, 1:len_seq + 1, 3] = seq[:, 2]\n",
        "            # $p_3$\n",
        "            self.data[i, len_seq + 1:, 4] = 1\n",
        "            # Mask is on until end of sequence\n",
        "            self.mask[i, :len_seq + 1] = 1\n",
        "\n",
        "        # Start-of-sequence is $(0, 0, 1, 0, 0)$\n",
        "        self.data[:, 0, 2] = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Size of the dataset\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"Get a sample\"\"\"\n",
        "        return self.data[idx], self.mask[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibb7Lsym-V96"
      },
      "outputs": [],
      "source": [
        "class BivariateGaussianMixture:\n",
        "    \"\"\"\n",
        "    ## Bi-variate Gaussian mixture\n",
        "    The mixture is represented by $\\Pi$ and\n",
        "    $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
        "    This class adjusts temperatures and creates the categorical and Gaussian\n",
        "    distributions from the parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pi_logits: torch.Tensor, mu_x: torch.Tensor, mu_y: torch.Tensor,\n",
        "                 sigma_x: torch.Tensor, sigma_y: torch.Tensor, rho_xy: torch.Tensor):\n",
        "        self.pi_logits = pi_logits\n",
        "        self.mu_x = mu_x\n",
        "        self.mu_y = mu_y\n",
        "        self.sigma_x = sigma_x\n",
        "        self.sigma_y = sigma_y\n",
        "        self.rho_xy = rho_xy\n",
        "\n",
        "    @property\n",
        "    def n_distributions(self):\n",
        "        \"\"\"Number of distributions in the mixture, $M$\"\"\"\n",
        "        return self.pi_logits.shape[-1]\n",
        "\n",
        "    def set_temperature(self, temperature: float):\n",
        "        \"\"\"\n",
        "        Adjust by temperature $\\tau$\n",
        "        \"\"\"\n",
        "        # $$\\hat{\\Pi_k} \\leftarrow \\frac{\\hat{\\Pi_k}}{\\tau}$$\n",
        "        self.pi_logits /= temperature\n",
        "        # $$\\sigma^2_x \\leftarrow \\sigma^2_x \\tau$$\n",
        "        self.sigma_x *= math.sqrt(temperature)\n",
        "        # $$\\sigma^2_y \\leftarrow \\sigma^2_y \\tau$$\n",
        "        self.sigma_y *= math.sqrt(temperature)\n",
        "\n",
        "    def get_distribution(self):\n",
        "        # Clamp $\\sigma_x$, $\\sigma_y$ and $\\rho_{xy}$ to avoid getting `NaN`s\n",
        "        sigma_x = torch.clamp_min(self.sigma_x, 1e-5)\n",
        "        sigma_y = torch.clamp_min(self.sigma_y, 1e-5)\n",
        "        rho_xy = torch.clamp(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
        "\n",
        "        # Get means\n",
        "        mean = torch.stack([self.mu_x, self.mu_y], -1)\n",
        "        # Get covariance matrix\n",
        "        cov = torch.stack([\n",
        "            sigma_x * sigma_x, rho_xy * sigma_x * sigma_y,\n",
        "            rho_xy * sigma_x * sigma_y, sigma_y * sigma_y\n",
        "        ], -1)\n",
        "        cov = cov.view(*sigma_y.shape, 2, 2)\n",
        "\n",
        "        # Create bi-variate normal distribution.\n",
        "        #\n",
        "        # ðŸ“ It would be efficient to `scale_tril` matrix as `[[a, 0], [b, c]]`\n",
        "        # where\n",
        "        # $$a = \\sigma_x, b = \\rho_{xy} \\sigma_y, c = \\sigma_y \\sqrt{1 - \\rho^2_{xy}}$$.\n",
        "        # But for simplicity we use co-variance matrix.\n",
        "        # [This is a good resource](https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf)\n",
        "        # if you want to read up more about bi-variate distributions, their co-variance matrix,\n",
        "        # and probability density function.\n",
        "        multi_dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov)\n",
        "\n",
        "        # Create categorical distribution $\\Pi$ from logits\n",
        "        cat_dist = torch.distributions.Categorical(logits=self.pi_logits)\n",
        "\n",
        "        #\n",
        "        return cat_dist, multi_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "m7OuONF5-cWT"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Encoder module\n",
        "    This consists of a bidirectional LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_z: int):\n",
        "        super().__init__()\n",
        "        # Create a bidirectional LSTM taking a sequence of\n",
        "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$ as input.\n",
        "        self.filter_hp = torch.tensor([[-1, -1, -1],\n",
        "                                             [-1, 8, -1],\n",
        "                                             [-1, -1, -1]]).float()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels= 4, kernel_size = 2, stride =2)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 4, out_channels= 4, kernel_size = 2, stride =1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 4, out_channels= 8, kernel_size = 2, stride =2)\n",
        "        self.conv4 = nn.Conv2d(in_channels = 8, out_channels= 8, kernel_size = 2, stride =1)\n",
        "        self.conv5 = nn.Conv2d(in_channels = 8, out_channels= 8, kernel_size = 2, stride =2)\n",
        "        self.conv6 = nn.Conv2d(in_channels = 8, out_channels= 8, kernel_size = 2, stride =1)\n",
        "\n",
        "        # Head to get $\\mu$\n",
        "        self.mu_head = nn.Linear(128, d_z)\n",
        "        # Head to get $\\hat{\\sigma}$\n",
        "        self.sigma_head = nn.Linear(128, d_z)\n",
        "\n",
        "    def high_pass_filtering(self, img_in):\n",
        "        \"\"\"\n",
        "        high pass filtering\n",
        "        :param img_in: [N, H, W, 1]\n",
        "        :return: img_out: [N, H, W, 1]\n",
        "        \"\"\"\n",
        "        #self.filter_hp = tf.expand_dims(tf.expand_dims(self.filter_hp, -1), -1, name='hp_w')  # [3, 3, 1, 1]\n",
        "        self.filter_hp = self.filter_hp.unsqueeze(-1).unsqueeze(-1)\n",
        "        print(self.filter_hp.shape)\n",
        "        img_out = F.conv2d(img_in, self.filter_hp)\n",
        "        return img_out\n",
        "\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, state=None):\n",
        "        x = self.high_pass_filtering(inputs)\n",
        "        print(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.tanh(self.conv6(x))\n",
        "        print(x.shape)\n",
        "\n",
        "        # $\\mu$\n",
        "        mu = self.mu_head(x)\n",
        "        # $\\hat{\\sigma}$\n",
        "        sigma_hat = self.sigma_head(x)\n",
        "        # $\\sigma = \\exp(\\frac{\\hat{\\sigma}}{2})$\n",
        "        sigma = torch.exp(sigma_hat / 2.)\n",
        "\n",
        "        # Sample $z = \\mu + \\sigma \\cdot \\mathcal{N}(0, I)$\n",
        "        z = mu + sigma * torch.normal(mu.new_zeros(mu.shape), mu.new_ones(mu.shape))\n",
        "\n",
        "        #\n",
        "        return z, mu, sigma_hat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_array = torch.rand((1,48,48,1))\n",
        "encoder = EncoderRNN(128)\n",
        "encoder(test_array)"
      ],
      "metadata": {
        "id": "5CMTHQo3W6tK",
        "outputId": "98cb5447-3a85-4fb6-a322-275796a04206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 1, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-186-ab3d6f7933c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-185-74c9561922d8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_pass_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-185-74c9561922d8>\u001b[0m in \u001b[0;36mhigh_pass_filtering\u001b[0;34m(self, img_in)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_hp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_hp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_hp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimg_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_hp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 3, 1, 1], expected input[1, 48, 48, 1] to have 3 channels, but got 48 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlnI2Xkw-5qY"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Decoder module\n",
        "    This consists of a LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_z: int, dec_hidden_size: int, n_distributions: int):\n",
        "        super().__init__()\n",
        "        # LSTM takes $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ as input\n",
        "        self.lstm = nn.LSTM(d_z + 5, dec_hidden_size)\n",
        "\n",
        "        # Initial state of the LSTM is $[h_0; c_0] = \\tanh(W_{z}z + b_z)$.\n",
        "        # `init_state` is the linear transformation for this\n",
        "        self.init_state = nn.Linear(d_z, 2 * dec_hidden_size)\n",
        "\n",
        "        # This layer produces outputs for each of the `n_distributions`.\n",
        "        # Each distribution needs six parameters\n",
        "        # $(\\hat{\\Pi_i}, \\mu_{x_i}, \\mu_{y_i}, \\hat{\\sigma_{x_i}}, \\hat{\\sigma_{y_i}} \\hat{\\rho_{xy_i}})$\n",
        "        self.mixtures = nn.Linear(dec_hidden_size, 6 * n_distributions)\n",
        "\n",
        "        # This head is for the logits $(\\hat{q_1}, \\hat{q_2}, \\hat{q_3})$\n",
        "        self.q_head = nn.Linear(dec_hidden_size, 3)\n",
        "        # This is to calculate $\\log(q_k)$ where\n",
        "        # $$q_k = \\operatorname{softmax}(\\hat{q})_k = \\frac{\\exp(\\hat{q_k})}{\\sum_{j = 1}^3 \\exp(\\hat{q_j})}$$\n",
        "        self.q_log_softmax = nn.LogSoftmax(-1)\n",
        "\n",
        "        # These parameters are stored for future reference\n",
        "        self.n_distributions = n_distributions\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor, z: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        # Calculate the initial state\n",
        "        if state is None:\n",
        "            # $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
        "            h, c = torch.split(torch.tanh(self.init_state(z)), self.dec_hidden_size, 1)\n",
        "            # `h` and `c` have shapes `[batch_size, lstm_size]`. We want to shape them\n",
        "            # to `[1, batch_size, lstm_size]` because that's the shape used in LSTM.\n",
        "            state = (h.unsqueeze(0).contiguous(), c.unsqueeze(0).contiguous())\n",
        "\n",
        "        # Run the LSTM\n",
        "        outputs, state = self.lstm(x, state)\n",
        "\n",
        "        # Get $\\log(q)$\n",
        "        q_logits = self.q_log_softmax(self.q_head(outputs))\n",
        "\n",
        "        # Get $(\\hat{\\Pi_i}, \\mu_{x,i}, \\mu_{y,i}, \\hat{\\sigma_{x,i}},\n",
        "        # \\hat{\\sigma_{y,i}} \\hat{\\rho_{xy,i}})$.\n",
        "        # `torch.split` splits the output into 6 tensors of size `self.n_distribution`\n",
        "        # across dimension `2`.\n",
        "        pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy = \\\n",
        "            torch.split(self.mixtures(outputs), self.n_distributions, 2)\n",
        "\n",
        "        # Create a bi-variate Gaussian mixture\n",
        "        # $\\Pi$ and \n",
        "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
        "        # where\n",
        "        # $$\\sigma_{x,i} = \\exp(\\hat{\\sigma_{x,i}}), \\sigma_{y,i} = \\exp(\\hat{\\sigma_{y,i}}),\n",
        "        # \\rho_{xy,i} = \\tanh(\\hat{\\rho_{xy,i}})$$\n",
        "        # and\n",
        "        # $$\\Pi_i = \\operatorname{softmax}(\\hat{\\Pi})_i = \\frac{\\exp(\\hat{\\Pi_i})}{\\sum_{j = 1}^3 \\exp(\\hat{\\Pi_j})}$$\n",
        "        #\n",
        "        # $\\Pi$ is the categorical probabilities of choosing the distribution out of the mixture\n",
        "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
        "        dist = BivariateGaussianMixture(pi_logits, mu_x, mu_y,\n",
        "                                        torch.exp(sigma_x), torch.exp(sigma_y), torch.tanh(rho_xy))\n",
        "\n",
        "        #\n",
        "        return dist, q_logits, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8NG3ZGw_A5d"
      },
      "outputs": [],
      "source": [
        "class ReconstructionLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Reconstruction Loss\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, mask: torch.Tensor, target: torch.Tensor,\n",
        "                 dist: 'BivariateGaussianMixture', q_logits: torch.Tensor):\n",
        "        # Get $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
        "        pi, mix = dist.get_distribution()\n",
        "        # `target` has shape `[seq_len, batch_size, 5]` where the last dimension is the features\n",
        "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
        "        # We want to get $\\Delta x, \\Delta$ y and get the probabilities from each of the distributions\n",
        "        # in the mixture $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
        "        #\n",
        "        # `xy` will have shape `[seq_len, batch_size, n_distributions, 2]`\n",
        "        xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
        "        # Calculate the probabilities\n",
        "        # $$p(\\Delta x, \\Delta y) =\n",
        "        # \\sum_{j=1}^M \\Pi_j \\mathcal{N} \\big( \\Delta x, \\Delta y \\vert\n",
        "        # \\mu_{x,j}, \\mu_{y,j}, \\sigma_{x,j}, \\sigma_{y,j}, \\rho_{xy,j}\n",
        "        # \\big)$$\n",
        "        probs = torch.sum(pi.probs * torch.exp(mix.log_prob(xy)), 2)\n",
        "\n",
        "        # $$L_s = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_s} \\log \\big (p(\\Delta x, \\Delta y) \\big)$$\n",
        "        # Although `probs` has $N_{max}$ (`longest_seq_len`) elements, the sum is only taken\n",
        "        # upto $N_s$ because the rest is masked out.\n",
        "        #\n",
        "        # It might feel like we should be taking the sum and dividing by $N_s$ and not $N_{max}$,\n",
        "        # but this will give higher weight for individual predictions in shorter sequences.\n",
        "        # We give equal weight to each prediction $p(\\Delta x, \\Delta y)$ when we divide by $N_{max}$\n",
        "        loss_stroke = -torch.mean(mask * torch.log(1e-5 + probs))\n",
        "\n",
        "        # $$L_p = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_{max}} \\sum_{k=1}^{3} p_{k,i} \\log(q_{k,i})$$\n",
        "        loss_pen = -torch.mean(target[:, :, 2:] * q_logits)\n",
        "\n",
        "        # $$L_R = L_s + L_p$$\n",
        "        return loss_stroke + loss_pen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42G58c1T_HNr"
      },
      "outputs": [],
      "source": [
        "class KLDivLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    ## KL-Divergence loss\n",
        "    This calculates the KL divergence between a given normal distribution and $\\mathcal{N}(0, 1)$\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, sigma_hat: torch.Tensor, mu: torch.Tensor):\n",
        "        # $$L_{KL} = - \\frac{1}{2 N_z} \\bigg( 1 + \\hat{\\sigma} - \\mu^2 - \\exp(\\hat{\\sigma}) \\bigg)$$\n",
        "        return -0.5 * torch.mean(1 + sigma_hat - mu ** 2 - torch.exp(sigma_hat))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh7206SX_NV7"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "    \"\"\"\n",
        "    ## Sampler\n",
        "    This samples a sketch from the decoder and plots it\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN):\n",
        "        self.decoder = decoder\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def sample(self, data: torch.Tensor, temperature: float):\n",
        "        # $N_{max}$\n",
        "        longest_seq_len = len(data)\n",
        "\n",
        "        # Get $z$ from the encoder\n",
        "        z, _, _ = self.encoder(data)\n",
        "\n",
        "        # Start-of-sequence stroke is $(0, 0, 1, 0, 0)$\n",
        "        s = data.new_tensor([0, 0, 1, 0, 0])\n",
        "        seq = [s]\n",
        "        # Initial decoder is `None`.\n",
        "        # The decoder will initialize it to $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
        "        state = None\n",
        "\n",
        "        # We don't need gradients\n",
        "        with torch.no_grad():\n",
        "            # Sample $N_{max}$ strokes\n",
        "            for i in range(longest_seq_len):\n",
        "                # $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ is the input to the decoder\n",
        "                data = torch.cat([s.view(1, 1, -1), z.unsqueeze(0)], 2)\n",
        "                # Get $\\Pi$, $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$,\n",
        "                # $q$ and the next state from the decoder\n",
        "                dist, q_logits, state = self.decoder(data, z, state)\n",
        "                # Sample a stroke\n",
        "                s = self._sample_step(dist, q_logits, temperature)\n",
        "                # Add the new stroke to the sequence of strokes\n",
        "                seq.append(s)\n",
        "                # Stop sampling if $p_3 = 1$. This indicates that sketching has stopped\n",
        "                if s[4] == 1:\n",
        "                    break\n",
        "\n",
        "        # Create a PyTorch tensor of the sequence of strokes\n",
        "        seq = torch.stack(seq)\n",
        "\n",
        "        # Plot the sequence of strokes\n",
        "        self.plot(seq)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample_step(dist: 'BivariateGaussianMixture', q_logits: torch.Tensor, temperature: float):\n",
        "        # Set temperature $\\tau$ for sampling. This is implemented in class `BivariateGaussianMixture`.\n",
        "        dist.set_temperature(temperature)\n",
        "        # Get temperature adjusted $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
        "        pi, mix = dist.get_distribution()\n",
        "        # Sample from $\\Pi$ the index of the distribution to use from the mixture\n",
        "        idx = pi.sample()[0, 0]\n",
        "\n",
        "        # Create categorical distribution $q$ with log-probabilities `q_logits` or $\\hat{q}$\n",
        "        q = torch.distributions.Categorical(logits=q_logits / temperature)\n",
        "        # Sample from $q$\n",
        "        q_idx = q.sample()[0, 0]\n",
        "\n",
        "        # Sample from the normal distributions in the mixture and pick the one indexed by `idx`\n",
        "        xy = mix.sample()[0, 0, idx]\n",
        "\n",
        "        # Create an empty stroke $(\\Delta x, \\Delta y, q_1, q_2, q_3)$\n",
        "        stroke = q_logits.new_zeros(5)\n",
        "        # Set $\\Delta x, \\Delta y$\n",
        "        stroke[:2] = xy\n",
        "        # Set $q_1, q_2, q_3$\n",
        "        stroke[q_idx + 2] = 1\n",
        "        #\n",
        "        return stroke\n",
        "\n",
        "    @staticmethod\n",
        "    def plot(seq: torch.Tensor):\n",
        "        # Take the cumulative sums of $(\\Delta x, \\Delta y)$ to get $(x, y)$\n",
        "        seq[:, 0:2] = torch.cumsum(seq[:, 0:2], dim=0)\n",
        "        # Create a new numpy array of the form $(x, y, q_2)$\n",
        "        seq[:, 2] = seq[:, 3]\n",
        "        seq = seq[:, 0:3].detach().cpu().numpy()\n",
        "\n",
        "        # Split the array at points where $q_2$ is $1$.\n",
        "        # i.e. split the array of strokes at the points where the pen is lifted from the paper.\n",
        "        # This gives a list of sequence of strokes.\n",
        "        strokes = np.split(seq, np.where(seq[:, 2] > 0)[0] + 1)\n",
        "        # Plot each sequence of strokes\n",
        "        for s in strokes:\n",
        "            plt.plot(s[:, 0], -s[:, 1])\n",
        "        # Don't show axes\n",
        "        plt.axis('off')\n",
        "        # Show the plot\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sZr9sB0_VpL"
      },
      "outputs": [],
      "source": [
        "class Configs():\n",
        "    \"\"\"\n",
        "    ## Configurations\n",
        "    These are default configurations which can later be adjusted by passing a `dict`.\n",
        "    \"\"\"\n",
        "\n",
        "    # Device configurations to pick the device to run the experiment\n",
        "    #device: torch.device = DeviceConfigs()\n",
        "    #\n",
        "    encoder: EncoderRNN\n",
        "    decoder: DecoderRNN\n",
        "    optimizer: optim.Adam\n",
        "    sampler: Sampler\n",
        "\n",
        "    dataset_name: str\n",
        "    train_loader: DataLoader\n",
        "    valid_loader: DataLoader\n",
        "    train_dataset: StrokesDataset\n",
        "    valid_dataset: StrokesDataset\n",
        "\n",
        "    # Encoder and decoder sizes\n",
        "    enc_hidden_size = 256\n",
        "    dec_hidden_size = 512\n",
        "\n",
        "    # Batch size\n",
        "    batch_size = 100\n",
        "\n",
        "    # Number of features in $z$\n",
        "    d_z = 128\n",
        "    # Number of distributions in the mixture, $M$\n",
        "    n_distributions = 20\n",
        "\n",
        "    # Weight of KL divergence loss, $w_{KL}$\n",
        "    kl_div_loss_weight = 0.5\n",
        "    # Gradient clipping\n",
        "    grad_clip = 1.\n",
        "    # Temperature $\\tau$ for sampling\n",
        "    temperature = 0.4\n",
        "\n",
        "    # Filter out stroke sequences longer than $200$\n",
        "    max_seq_length = 200\n",
        "\n",
        "    epochs = 100\n",
        "\n",
        "    kl_div_loss = KLDivLoss()\n",
        "    reconstruction_loss = ReconstructionLoss()\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda'\n",
        "        # Initialize encoder & decoder\n",
        "        self.encoder = EncoderRNN(self.d_z, self.enc_hidden_size).to(self.device)\n",
        "        self.decoder = DecoderRNN(self.d_z, self.dec_hidden_size, self.n_distributions).to(self.device)\n",
        "\n",
        "        # Set optimizer. Things like type of optimizer and learning rate are configurable\n",
        "        #optimizer = OptimizerConfigs()\n",
        "        #optimizer.parameters = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
        "        #self.optimizer = optimizer\n",
        "        self.optimizer = optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=0.0001)\n",
        "\n",
        "        # Create sampler\n",
        "        self.sampler = Sampler(self.encoder, self.decoder)\n",
        "\n",
        "        # `npz` file path is `data/sketch/[DATASET NAME].npz`\n",
        "        path = 'cat.npz'\n",
        "        # Load the numpy file\n",
        "        dataset = np.load(str(path), encoding='latin1', allow_pickle=True)\n",
        "\n",
        "        # Create training dataset\n",
        "        self.train_dataset = StrokesDataset(dataset['train'], self.max_seq_length)\n",
        "        # Create validation dataset\n",
        "        self.valid_dataset = StrokesDataset(dataset['valid'], self.max_seq_length, self.train_dataset.scale)\n",
        "\n",
        "        # Create training data loader\n",
        "        self.train_loader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
        "        # Create validation data loader\n",
        "        self.valid_loader = DataLoader(self.valid_dataset, self.batch_size)\n",
        "\n",
        "        self.state_modules = []\n",
        "\n",
        "    def step(self, batch: Any, is_train=True):\n",
        "        self.encoder.train(is_train)\n",
        "        self.decoder.train(is_train)\n",
        "\n",
        "        # Move `data` and `mask` to device and swap the sequence and batch dimensions.\n",
        "        # `data` will have shape `[seq_len, batch_size, 5]` and\n",
        "        # `mask` will have shape `[seq_len, batch_size]`.\n",
        "        data = batch[0].to(self.device).transpose(0, 1)\n",
        "        mask = batch[1].to(self.device).transpose(0, 1)\n",
        "\n",
        "\n",
        "\n",
        "        z, mu, sigma_hat = self.encoder(data)\n",
        "\n",
        "        # Decode the mixture of distributions and $\\hat{q}$\n",
        "\n",
        "        # Concatenate $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$\n",
        "        z_stack = z.unsqueeze(0).expand(data.shape[0] - 1, -1, -1)\n",
        "        inputs = torch.cat([data[:-1], z_stack], 2)\n",
        "        # Get mixture of distributions and $\\hat{q}$\n",
        "        dist, q_logits, _ = self.decoder(inputs, z, None)\n",
        "\n",
        "        # Compute the loss\n",
        "        # $L_{KL}$\n",
        "        kl_loss = self.kl_div_loss(sigma_hat, mu)\n",
        "        # $L_R$\n",
        "        reconstruction_loss = self.reconstruction_loss(mask, data[1:], dist, q_logits)\n",
        "        # $Loss = L_R + w_{KL} L_{KL}$\n",
        "        loss = reconstruction_loss + self.kl_div_loss_weight * kl_loss\n",
        "\n",
        "\n",
        "\n",
        "        # Only if we are in training state\n",
        "        if is_train:\n",
        "\n",
        "            # Set `grad` to zero\n",
        "            self.optimizer.zero_grad()\n",
        "            # Compute gradients\n",
        "            loss.backward()\n",
        "            #print(loss.item())\n",
        "            # Clip gradients\n",
        "            nn.utils.clip_grad_norm_(self.encoder.parameters(), self.grad_clip)\n",
        "            nn.utils.clip_grad_norm_(self.decoder.parameters(), self.grad_clip)\n",
        "            # Optimize\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        # Randomly pick a sample from validation dataset to encoder\n",
        "        data, *_ = self.valid_dataset[np.random.choice(len(self.valid_dataset))]\n",
        "        # Add batch dimension and move it to device\n",
        "        data = data.unsqueeze(1).to(self.device)\n",
        "        # Sample\n",
        "        self.sampler.sample(data, self.temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPY1E43sAnhb"
      },
      "outputs": [],
      "source": [
        "c = Configs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNJILEbgBjiP"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 100\n",
        "\n",
        "for i in range(epochs):\n",
        "  for batch in tqdm(c.train_loader):\n",
        "    c.step(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g8gchrtEOOx"
      },
      "outputs": [],
      "source": [
        "\n",
        "c.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqHL-qF7IIWO"
      },
      "outputs": [],
      "source": [
        "torch.save(c.encoder.state_dict(), './encoder_weights')\n",
        "torch.save(c.decoder.state_dict(), './decoder_weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gALFq2jKBP3o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}