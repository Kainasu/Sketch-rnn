{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1CO1A6ZlGtN-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-30 14:01:18--  https://storage.googleapis.com/quickdraw_dataset/sketchrnn/cat.npz\n",
            "Résolution de storage.googleapis.com (storage.googleapis.com)… 142.250.201.16, 216.58.205.208, 142.251.37.176, ...\n",
            "Connexion à storage.googleapis.com (storage.googleapis.com)|142.250.201.16|:443… connecté.\n",
            "requête HTTP transmise, en attente de la réponse… 200 OK\n",
            "Taille : 14842947 (14M) [application/octet-stream]\n",
            "Enregistre : ‘cat.npz’\n",
            "\n",
            "cat.npz             100%[===================>]  14,16M  11,0MB/s    ds 1,3s    \n",
            "\n",
            "2022-11-30 14:01:20 (11,0 MB/s) - ‘cat.npz’ enregistré [14842947/14842947]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/quickdraw_dataset/sketchrnn/cat.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4GvzWJkDDNT",
        "outputId": "dd5a6afc-e49e-45ba-c8b4-0798b4fcc6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: einops in /home/ndo001/.local/lib/python3.10/site-packages (0.6.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gbzcD-m49_dP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xSgcOTcP-Oag"
      },
      "outputs": [],
      "source": [
        "class StrokesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    ## Dataset\n",
        "    This class loads and pre-processes the data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset: np.array, max_seq_length: int, scale: Optional[float] = None):\n",
        "        \"\"\"\n",
        "        `dataset` is a list of numpy arrays of shape [seq_len, 3].\n",
        "        It is a sequence of strokes, and each stroke is represented by\n",
        "        3 integers.\n",
        "        First two are the displacements along x and y ($\\Delta x$, $\\Delta y$)\n",
        "        and the last integer represents the state of the pen, $1$ if it's touching\n",
        "        the paper and $0$ otherwise.\n",
        "        \"\"\"\n",
        "\n",
        "        data = []\n",
        "        # We iterate through each of the sequences and filter\n",
        "        for seq in dataset:\n",
        "            # Filter if the length of the sequence of strokes is within our range\n",
        "            if 10 < len(seq) <= max_seq_length:\n",
        "                # Clamp $\\Delta x$, $\\Delta y$ to $[-1000, 1000]$\n",
        "                seq = np.minimum(seq, 1000)\n",
        "                seq = np.maximum(seq, -1000)\n",
        "                # Convert to a floating point array and add to `data`\n",
        "                seq = np.array(seq, dtype=np.float32)\n",
        "                data.append(seq)\n",
        "\n",
        "        # We then calculate the scaling factor which is the\n",
        "        # standard deviation of ($\\Delta x$, $\\Delta y$) combined.\n",
        "        # Paper notes that the mean is not adjusted for simplicity,\n",
        "        # since the mean is anyway close to $0$.\n",
        "        if scale is None:\n",
        "            scale = np.std(np.concatenate([np.ravel(s[:, 0:2]) for s in data]))\n",
        "        self.scale = scale\n",
        "\n",
        "        # Get the longest sequence length among all sequences\n",
        "        longest_seq_len = max([len(seq) for seq in data])\n",
        "\n",
        "        # We initialize PyTorch data array with two extra steps for start-of-sequence (sos)\n",
        "        # and end-of-sequence (eos).\n",
        "        # Each step is a vector $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
        "        # Only one of $p_1, p_2, p_3$ is $1$ and the others are $0$.\n",
        "        # They represent *pen down*, *pen up* and *end-of-sequence* in that order.\n",
        "        # $p_1$ is $1$ if the pen touches the paper in the next step.\n",
        "        # $p_2$ is $1$ if the pen doesn't touch the paper in the next step.\n",
        "        # $p_3$ is $1$ if it is the end of the drawing.\n",
        "        self.data = torch.zeros(len(data), longest_seq_len + 2, 5, dtype=torch.float)\n",
        "        # The mask array needs only one extra-step since it is for the outputs of the\n",
        "        # decoder, which takes in `data[:-1]` and predicts next step.\n",
        "        self.mask = torch.zeros(len(data), longest_seq_len + 1)\n",
        "\n",
        "        for i, seq in enumerate(data):\n",
        "            seq = torch.from_numpy(seq)\n",
        "            len_seq = len(seq)\n",
        "            # Scale and set $\\Delta x, \\Delta y$\n",
        "            self.data[i, 1:len_seq + 1, :2] = seq[:, :2] / scale\n",
        "            # $p_1$\n",
        "            self.data[i, 1:len_seq + 1, 2] = 1 - seq[:, 2]\n",
        "            # $p_2$\n",
        "            self.data[i, 1:len_seq + 1, 3] = seq[:, 2]\n",
        "            # $p_3$\n",
        "            self.data[i, len_seq + 1:, 4] = 1\n",
        "            # Mask is on until end of sequence\n",
        "            self.mask[i, :len_seq + 1] = 1\n",
        "\n",
        "        # Start-of-sequence is $(0, 0, 1, 0, 0)$\n",
        "        self.data[:, 0, 2] = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Size of the dataset\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"Get a sample\"\"\"\n",
        "        return self.data[idx], self.mask[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ibb7Lsym-V96"
      },
      "outputs": [],
      "source": [
        "class BivariateGaussianMixture:\n",
        "    \"\"\"\n",
        "    ## Bi-variate Gaussian mixture\n",
        "    The mixture is represented by $\\Pi$ and\n",
        "    $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
        "    This class adjusts temperatures and creates the categorical and Gaussian\n",
        "    distributions from the parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pi_logits: torch.Tensor, mu_x: torch.Tensor, mu_y: torch.Tensor,\n",
        "                 sigma_x: torch.Tensor, sigma_y: torch.Tensor, rho_xy: torch.Tensor):\n",
        "        self.pi_logits = pi_logits\n",
        "        self.mu_x = mu_x\n",
        "        self.mu_y = mu_y\n",
        "        self.sigma_x = sigma_x\n",
        "        self.sigma_y = sigma_y\n",
        "        self.rho_xy = rho_xy\n",
        "\n",
        "    @property\n",
        "    def n_distributions(self):\n",
        "        \"\"\"Number of distributions in the mixture, $M$\"\"\"\n",
        "        return self.pi_logits.shape[-1]\n",
        "\n",
        "    def set_temperature(self, temperature: float):\n",
        "        \"\"\"\n",
        "        Adjust by temperature $\\tau$\n",
        "        \"\"\"\n",
        "        # $$\\hat{\\Pi_k} \\leftarrow \\frac{\\hat{\\Pi_k}}{\\tau}$$\n",
        "        self.pi_logits /= temperature\n",
        "        # $$\\sigma^2_x \\leftarrow \\sigma^2_x \\tau$$\n",
        "        self.sigma_x *= math.sqrt(temperature)\n",
        "        # $$\\sigma^2_y \\leftarrow \\sigma^2_y \\tau$$\n",
        "        self.sigma_y *= math.sqrt(temperature)\n",
        "\n",
        "    def get_distribution(self):\n",
        "        # Clamp $\\sigma_x$, $\\sigma_y$ and $\\rho_{xy}$ to avoid getting `NaN`s\n",
        "        sigma_x = torch.clamp_min(self.sigma_x, 1e-5)\n",
        "        sigma_y = torch.clamp_min(self.sigma_y, 1e-5)\n",
        "        rho_xy = torch.clamp(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
        "\n",
        "        # Get means\n",
        "        mean = torch.stack([self.mu_x, self.mu_y], -1)\n",
        "        # Get covariance matrix\n",
        "        cov = torch.stack([\n",
        "            sigma_x * sigma_x, rho_xy * sigma_x * sigma_y,\n",
        "            rho_xy * sigma_x * sigma_y, sigma_y * sigma_y\n",
        "        ], -1)\n",
        "        cov = cov.view(*sigma_y.shape, 2, 2)\n",
        "\n",
        "        # Create bi-variate normal distribution.\n",
        "        #\n",
        "        # 📝 It would be efficient to `scale_tril` matrix as `[[a, 0], [b, c]]`\n",
        "        # where\n",
        "        # $$a = \\sigma_x, b = \\rho_{xy} \\sigma_y, c = \\sigma_y \\sqrt{1 - \\rho^2_{xy}}$$.\n",
        "        # But for simplicity we use co-variance matrix.\n",
        "        # [This is a good resource](https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf)\n",
        "        # if you want to read up more about bi-variate distributions, their co-variance matrix,\n",
        "        # and probability density function.\n",
        "        multi_dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov)\n",
        "\n",
        "        # Create categorical distribution $\\Pi$ from logits\n",
        "        cat_dist = torch.distributions.Categorical(logits=self.pi_logits)\n",
        "\n",
        "        #\n",
        "        return cat_dist, multi_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m7OuONF5-cWT"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Encoder module\n",
        "    This consists of a bidirectional LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_z: int, enc_hidden_size: int):\n",
        "        super().__init__()\n",
        "        # Create a bidirectional LSTM taking a sequence of\n",
        "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$ as input.\n",
        "        self.lstm = nn.LSTM(5, enc_hidden_size, bidirectional=True)\n",
        "        # Head to get $\\mu$\n",
        "        self.mu_head = nn.Linear(2 * enc_hidden_size, d_z)\n",
        "        # Head to get $\\hat{\\sigma}$\n",
        "        self.sigma_head = nn.Linear(2 * enc_hidden_size, d_z)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, state=None):\n",
        "        # The hidden state of the bidirectional LSTM is the concatenation of the\n",
        "        # output of the last token in the forward direction and\n",
        "        # first token in the reverse direction, which is what we want.\n",
        "        # $$h_{\\rightarrow} = encode_{\\rightarrow}(S),\n",
        "        # h_{\\leftarrow} = encode←_{\\leftarrow}(S_{reverse}),\n",
        "        # h = [h_{\\rightarrow}; h_{\\leftarrow}]$$\n",
        "        _, (hidden, cell) = self.lstm(inputs.float(), state)\n",
        "        # The state has shape `[2, batch_size, hidden_size]`,\n",
        "        # where the first dimension is the direction.\n",
        "        # We rearrange it to get $h = [h_{\\rightarrow}; h_{\\leftarrow}]$\n",
        "        hidden = einops.rearrange(hidden, 'fb b h -> b (fb h)')\n",
        "\n",
        "        # $\\mu$\n",
        "        mu = self.mu_head(hidden)\n",
        "        # $\\hat{\\sigma}$\n",
        "        sigma_hat = self.sigma_head(hidden)\n",
        "        # $\\sigma = \\exp(\\frac{\\hat{\\sigma}}{2})$\n",
        "        sigma = torch.exp(sigma_hat / 2.)\n",
        "\n",
        "        # Sample $z = \\mu + \\sigma \\cdot \\mathcal{N}(0, I)$\n",
        "        z = mu + sigma * torch.normal(mu.new_zeros(mu.shape), mu.new_ones(mu.shape))\n",
        "\n",
        "        #\n",
        "        return z, mu, sigma_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 9, 9])\n",
            "torch.Size([648])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([0.8708], grad_fn=<AddBackward0>),\n",
              " tensor([0.0062], grad_fn=<AddBackward0>),\n",
              " tensor([-0.0511], grad_fn=<AddBackward0>))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class EncoderRNN_v2(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Encoder module\n",
        "    This consists of a CNN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_z: int):\n",
        "        super().__init__()\n",
        "        self.high_pass_filter = torch.tensor([[-1, -1, -1],\n",
        "                                              [-1, 8, -1],\n",
        "                                              [-1, -1, -1]])\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=4, kernel_size=2, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=2, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=2, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=2, stride=2, padding=1)\n",
        "        self.conv6 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "        # Head to get $\\mu$\n",
        "        self.mu_head = nn.Linear(648, d_z)\n",
        "        # Head to get $\\hat{\\sigma}$\n",
        "        self.sigma_head = nn.Linear(648, d_z)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, state=None):\n",
        "        x = F.relu(self.conv1(inputs))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.tanh(self.conv6(x))\n",
        "        \n",
        "        # \n",
        "        x = x.view(-1)\n",
        "\n",
        "        # $\\mu$\n",
        "        mu = self.mu_head(x)\n",
        "        # $\\hat{\\sigma}$\n",
        "        sigma_hat = self.sigma_head(x)\n",
        "        # $\\sigma = \\exp(\\frac{\\hat{\\sigma}}{2})$\n",
        "        sigma = torch.exp(sigma_hat / 2.)\n",
        "\n",
        "        # Sample $z = \\mu + \\sigma \\cdot \\mathcal{N}(0, I)$\n",
        "        z = mu + sigma * torch.normal(mu.new_zeros(mu.shape), mu.new_ones(mu.shape))\n",
        "\n",
        "        #\n",
        "        return z, mu, sigma_hat\n",
        "\n",
        "# encoder = EncoderRNN_v2(1)\n",
        "# inputs = torch.rand((1, 48,48))\n",
        "# encoder(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HlnI2Xkw-5qY"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Decoder module\n",
        "    This consists of a LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_z: int, dec_hidden_size: int, n_distributions: int):\n",
        "        super().__init__()\n",
        "        # LSTM takes $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ as input\n",
        "        self.lstm = nn.LSTM(d_z + 5, dec_hidden_size)\n",
        "\n",
        "        # Initial state of the LSTM is $[h_0; c_0] = \\tanh(W_{z}z + b_z)$.\n",
        "        # `init_state` is the linear transformation for this\n",
        "        self.init_state = nn.Linear(d_z, 2 * dec_hidden_size)\n",
        "\n",
        "        # This layer produces outputs for each of the `n_distributions`.\n",
        "        # Each distribution needs six parameters\n",
        "        # $(\\hat{\\Pi_i}, \\mu_{x_i}, \\mu_{y_i}, \\hat{\\sigma_{x_i}}, \\hat{\\sigma_{y_i}} \\hat{\\rho_{xy_i}})$\n",
        "        self.mixtures = nn.Linear(dec_hidden_size, 6 * n_distributions)\n",
        "\n",
        "        # This head is for the logits $(\\hat{q_1}, \\hat{q_2}, \\hat{q_3})$\n",
        "        self.q_head = nn.Linear(dec_hidden_size, 3)\n",
        "        # This is to calculate $\\log(q_k)$ where\n",
        "        # $$q_k = \\operatorname{softmax}(\\hat{q})_k = \\frac{\\exp(\\hat{q_k})}{\\sum_{j = 1}^3 \\exp(\\hat{q_j})}$$\n",
        "        self.q_log_softmax = nn.LogSoftmax(-1)\n",
        "\n",
        "        # These parameters are stored for future reference\n",
        "        self.n_distributions = n_distributions\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor, z: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        # Calculate the initial state\n",
        "        if state is None:\n",
        "            # $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
        "            h, c = torch.split(torch.tanh(self.init_state(z)), self.dec_hidden_size, 1)\n",
        "            # `h` and `c` have shapes `[batch_size, lstm_size]`. We want to shape them\n",
        "            # to `[1, batch_size, lstm_size]` because that's the shape used in LSTM.\n",
        "            state = (h.unsqueeze(0).contiguous(), c.unsqueeze(0).contiguous())\n",
        "\n",
        "        # Run the LSTM\n",
        "        outputs, state = self.lstm(x, state)\n",
        "\n",
        "        # Get $\\log(q)$\n",
        "        q_logits = self.q_log_softmax(self.q_head(outputs))\n",
        "\n",
        "        # Get $(\\hat{\\Pi_i}, \\mu_{x,i}, \\mu_{y,i}, \\hat{\\sigma_{x,i}},\n",
        "        # \\hat{\\sigma_{y,i}} \\hat{\\rho_{xy,i}})$.\n",
        "        # `torch.split` splits the output into 6 tensors of size `self.n_distribution`\n",
        "        # across dimension `2`.\n",
        "        pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy = \\\n",
        "            torch.split(self.mixtures(outputs), self.n_distributions, 2)\n",
        "\n",
        "        # Create a bi-variate Gaussian mixture\n",
        "        # $\\Pi$ and \n",
        "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
        "        # where\n",
        "        # $$\\sigma_{x,i} = \\exp(\\hat{\\sigma_{x,i}}), \\sigma_{y,i} = \\exp(\\hat{\\sigma_{y,i}}),\n",
        "        # \\rho_{xy,i} = \\tanh(\\hat{\\rho_{xy,i}})$$\n",
        "        # and\n",
        "        # $$\\Pi_i = \\operatorname{softmax}(\\hat{\\Pi})_i = \\frac{\\exp(\\hat{\\Pi_i})}{\\sum_{j = 1}^3 \\exp(\\hat{\\Pi_j})}$$\n",
        "        #\n",
        "        # $\\Pi$ is the categorical probabilities of choosing the distribution out of the mixture\n",
        "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
        "        dist = BivariateGaussianMixture(pi_logits, mu_x, mu_y,\n",
        "                                        torch.exp(sigma_x), torch.exp(sigma_y), torch.tanh(rho_xy))\n",
        "\n",
        "        #\n",
        "        return dist, q_logits, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g8NG3ZGw_A5d"
      },
      "outputs": [],
      "source": [
        "class ReconstructionLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Reconstruction Loss\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, mask: torch.Tensor, target: torch.Tensor,\n",
        "                 dist: 'BivariateGaussianMixture', q_logits: torch.Tensor):\n",
        "        # Get $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
        "        pi, mix = dist.get_distribution()\n",
        "        # `target` has shape `[seq_len, batch_size, 5]` where the last dimension is the features\n",
        "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
        "        # We want to get $\\Delta x, \\Delta$ y and get the probabilities from each of the distributions\n",
        "        # in the mixture $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
        "        #\n",
        "        # `xy` will have shape `[seq_len, batch_size, n_distributions, 2]`\n",
        "        xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
        "        # Calculate the probabilities\n",
        "        # $$p(\\Delta x, \\Delta y) =\n",
        "        # \\sum_{j=1}^M \\Pi_j \\mathcal{N} \\big( \\Delta x, \\Delta y \\vert\n",
        "        # \\mu_{x,j}, \\mu_{y,j}, \\sigma_{x,j}, \\sigma_{y,j}, \\rho_{xy,j}\n",
        "        # \\big)$$\n",
        "        probs = torch.sum(pi.probs * torch.exp(mix.log_prob(xy)), 2)\n",
        "\n",
        "        # $$L_s = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_s} \\log \\big (p(\\Delta x, \\Delta y) \\big)$$\n",
        "        # Although `probs` has $N_{max}$ (`longest_seq_len`) elements, the sum is only taken\n",
        "        # upto $N_s$ because the rest is masked out.\n",
        "        #\n",
        "        # It might feel like we should be taking the sum and dividing by $N_s$ and not $N_{max}$,\n",
        "        # but this will give higher weight for individual predictions in shorter sequences.\n",
        "        # We give equal weight to each prediction $p(\\Delta x, \\Delta y)$ when we divide by $N_{max}$\n",
        "        loss_stroke = -torch.mean(mask * torch.log(1e-5 + probs))\n",
        "\n",
        "        # $$L_p = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_{max}} \\sum_{k=1}^{3} p_{k,i} \\log(q_{k,i})$$\n",
        "        loss_pen = -torch.mean(target[:, :, 2:] * q_logits)\n",
        "\n",
        "        # $$L_R = L_s + L_p$$\n",
        "        return loss_stroke + loss_pen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "42G58c1T_HNr"
      },
      "outputs": [],
      "source": [
        "class KLDivLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    ## KL-Divergence loss\n",
        "    This calculates the KL divergence between a given normal distribution and $\\mathcal{N}(0, 1)$\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, sigma_hat: torch.Tensor, mu: torch.Tensor):\n",
        "        # $$L_{KL} = - \\frac{1}{2 N_z} \\bigg( 1 + \\hat{\\sigma} - \\mu^2 - \\exp(\\hat{\\sigma}) \\bigg)$$\n",
        "        return -0.5 * torch.mean(1 + sigma_hat - mu ** 2 - torch.exp(sigma_hat))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pix2seqLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    ## KPix2seq loss    \n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, sigma_hat: torch.Tensor, mu: torch.Tensor):\n",
        "        # $$L_{KL} = - \\frac{1}{2 N_z} \\bigg( 1 + \\hat{\\sigma} - \\mu^2 - \\exp(\\hat{\\sigma}) \\bigg)$$\n",
        "        return -0.5 * torch.mean(1 + sigma_hat - mu ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nh7206SX_NV7"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "    \"\"\"\n",
        "    ## Sampler\n",
        "    This samples a sketch from the decoder and plots it\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN):\n",
        "        self.decoder = decoder\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def sample(self, data: torch.Tensor, temperature: float):\n",
        "        # $N_{max}$\n",
        "        longest_seq_len = len(data)\n",
        "\n",
        "        # Get $z$ from the encoder\n",
        "        z, _, _ = self.encoder(data)\n",
        "\n",
        "        # Start-of-sequence stroke is $(0, 0, 1, 0, 0)$\n",
        "        s = data.new_tensor([0, 0, 1, 0, 0])\n",
        "        seq = [s]\n",
        "        # Initial decoder is `None`.\n",
        "        # The decoder will initialize it to $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
        "        state = None\n",
        "\n",
        "        # We don't need gradients\n",
        "        with torch.no_grad():\n",
        "            # Sample $N_{max}$ strokes\n",
        "            for i in range(longest_seq_len):\n",
        "                # $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ is the input to the decoder\n",
        "                data = torch.cat([s.view(1, 1, -1), z.unsqueeze(0)], 2)\n",
        "                # Get $\\Pi$, $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$,\n",
        "                # $q$ and the next state from the decoder\n",
        "                dist, q_logits, state = self.decoder(data, z, state)\n",
        "                # Sample a stroke\n",
        "                s = self._sample_step(dist, q_logits, temperature)\n",
        "                # Add the new stroke to the sequence of strokes\n",
        "                seq.append(s)\n",
        "                # Stop sampling if $p_3 = 1$. This indicates that sketching has stopped\n",
        "                if s[4] == 1:\n",
        "                    break\n",
        "\n",
        "        # Create a PyTorch tensor of the sequence of strokes\n",
        "        seq = torch.stack(seq)\n",
        "\n",
        "        # Plot the sequence of strokes\n",
        "        self.plot(seq)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample_step(dist: 'BivariateGaussianMixture', q_logits: torch.Tensor, temperature: float):\n",
        "        # Set temperature $\\tau$ for sampling. This is implemented in class `BivariateGaussianMixture`.\n",
        "        dist.set_temperature(temperature)\n",
        "        # Get temperature adjusted $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
        "        pi, mix = dist.get_distribution()\n",
        "        # Sample from $\\Pi$ the index of the distribution to use from the mixture\n",
        "        idx = pi.sample()[0, 0]\n",
        "\n",
        "        # Create categorical distribution $q$ with log-probabilities `q_logits` or $\\hat{q}$\n",
        "        q = torch.distributions.Categorical(logits=q_logits / temperature)\n",
        "        # Sample from $q$\n",
        "        q_idx = q.sample()[0, 0]\n",
        "\n",
        "        # Sample from the normal distributions in the mixture and pick the one indexed by `idx`\n",
        "        xy = mix.sample()[0, 0, idx]\n",
        "\n",
        "        # Create an empty stroke $(\\Delta x, \\Delta y, q_1, q_2, q_3)$\n",
        "        stroke = q_logits.new_zeros(5)\n",
        "        # Set $\\Delta x, \\Delta y$\n",
        "        stroke[:2] = xy\n",
        "        # Set $q_1, q_2, q_3$\n",
        "        stroke[q_idx + 2] = 1\n",
        "        #\n",
        "        return stroke\n",
        "\n",
        "    @staticmethod\n",
        "    def plot(seq: torch.Tensor):\n",
        "        # Take the cumulative sums of $(\\Delta x, \\Delta y)$ to get $(x, y)$\n",
        "        seq[:, 0:2] = torch.cumsum(seq[:, 0:2], dim=0)\n",
        "        # Create a new numpy array of the form $(x, y, q_2)$\n",
        "        seq[:, 2] = seq[:, 3]\n",
        "        seq = seq[:, 0:3].detach().cpu().numpy()\n",
        "\n",
        "        # Split the array at points where $q_2$ is $1$.\n",
        "        # i.e. split the array of strokes at the points where the pen is lifted from the paper.\n",
        "        # This gives a list of sequence of strokes.\n",
        "        strokes = np.split(seq, np.where(seq[:, 2] > 0)[0] + 1)\n",
        "        # Plot each sequence of strokes\n",
        "        for s in strokes:\n",
        "            plt.plot(s[:, 0], -s[:, 1])\n",
        "        # Don't show axes\n",
        "        plt.axis('off')\n",
        "        # Show the plot\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5sZr9sB0_VpL"
      },
      "outputs": [],
      "source": [
        "class Configs():\n",
        "    \"\"\"\n",
        "    ## Configurations\n",
        "    These are default configurations which can later be adjusted by passing a `dict`.\n",
        "    \"\"\"\n",
        "\n",
        "    # Device configurations to pick the device to run the experiment\n",
        "    #device: torch.device = DeviceConfigs()\n",
        "    #\n",
        "    encoder: EncoderRNN\n",
        "    decoder: DecoderRNN\n",
        "    optimizer: optim.Adam\n",
        "    sampler: Sampler\n",
        "\n",
        "    dataset_name: str\n",
        "    train_loader: DataLoader\n",
        "    valid_loader: DataLoader\n",
        "    train_dataset: StrokesDataset\n",
        "    valid_dataset: StrokesDataset\n",
        "\n",
        "    # Encoder and decoder sizes\n",
        "    enc_hidden_size = 256\n",
        "    dec_hidden_size = 512\n",
        "\n",
        "    # Batch size\n",
        "    batch_size = 100\n",
        "\n",
        "    # Number of features in $z$\n",
        "    d_z = 128\n",
        "    # Number of distributions in the mixture, $M$\n",
        "    n_distributions = 20\n",
        "\n",
        "    # Weight of KL divergence loss, $w_{KL}$\n",
        "    kl_div_loss_weight = 0.5\n",
        "    # Gradient clipping\n",
        "    grad_clip = 1.\n",
        "    # Temperature $\\tau$ for sampling\n",
        "    temperature = 0.4\n",
        "\n",
        "    # Filter out stroke sequences longer than $200$\n",
        "    max_seq_length = 200\n",
        "\n",
        "    epochs = 100\n",
        "\n",
        "    kl_div_loss = KLDivLoss()\n",
        "    reconstruction_loss = ReconstructionLoss()\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda'\n",
        "        # Initialize encoder & decoder\n",
        "        self.encoder = EncoderRNN(self.d_z, self.enc_hidden_size).to(self.device)\n",
        "        self.decoder = DecoderRNN(self.d_z, self.dec_hidden_size, self.n_distributions).to(self.device)\n",
        "\n",
        "        # Set optimizer. Things like type of optimizer and learning rate are configurable\n",
        "        #optimizer = OptimizerConfigs()\n",
        "        #optimizer.parameters = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
        "        #self.optimizer = optimizer\n",
        "        self.optimizer = optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=0.0001)\n",
        "\n",
        "        # Create sampler\n",
        "        self.sampler = Sampler(self.encoder, self.decoder)\n",
        "\n",
        "        # `npz` file path is `data/sketch/[DATASET NAME].npz`\n",
        "        path = 'cat.npz'\n",
        "        # Load the numpy file\n",
        "        dataset = np.load(str(path), encoding='latin1', allow_pickle=True)\n",
        "\n",
        "        # Create training dataset\n",
        "        self.train_dataset = StrokesDataset(dataset['train'], self.max_seq_length)\n",
        "        # Create validation dataset\n",
        "        self.valid_dataset = StrokesDataset(dataset['valid'], self.max_seq_length, self.train_dataset.scale)\n",
        "\n",
        "        # Create training data loader\n",
        "        self.train_loader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
        "        # Create validation data loader\n",
        "        self.valid_loader = DataLoader(self.valid_dataset, self.batch_size)\n",
        "\n",
        "        self.state_modules = []\n",
        "\n",
        "    def step(self, batch: Any, is_train=True):\n",
        "        self.encoder.train(is_train)\n",
        "        self.decoder.train(is_train)\n",
        "\n",
        "        # Move `data` and `mask` to device and swap the sequence and batch dimensions.\n",
        "        # `data` will have shape `[seq_len, batch_size, 5]` and\n",
        "        # `mask` will have shape `[seq_len, batch_size]`.\n",
        "        data = batch[0].to(self.device).transpose(0, 1)\n",
        "        mask = batch[1].to(self.device).transpose(0, 1)\n",
        "\n",
        "\n",
        "\n",
        "        z, mu, sigma_hat = self.encoder(data)\n",
        "\n",
        "        # Decode the mixture of distributions and $\\hat{q}$\n",
        "\n",
        "        # Concatenate $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$\n",
        "        z_stack = z.unsqueeze(0).expand(data.shape[0] - 1, -1, -1)\n",
        "        inputs = torch.cat([data[:-1], z_stack], 2)\n",
        "        # Get mixture of distributions and $\\hat{q}$\n",
        "        dist, q_logits, _ = self.decoder(inputs, z, None)\n",
        "\n",
        "        # Compute the loss\n",
        "        # $L_{KL}$\n",
        "        kl_loss = self.kl_div_loss(sigma_hat, mu)\n",
        "        # $L_R$\n",
        "        reconstruction_loss = self.reconstruction_loss(mask, data[1:], dist, q_logits)\n",
        "        # $Loss = L_R + w_{KL} L_{KL}$\n",
        "        loss = reconstruction_loss + self.kl_div_loss_weight * kl_loss\n",
        "\n",
        "\n",
        "\n",
        "        # Only if we are in training state\n",
        "        if is_train:\n",
        "\n",
        "            # Set `grad` to zero\n",
        "            self.optimizer.zero_grad()\n",
        "            # Compute gradients\n",
        "            loss.backward()\n",
        "            #print(loss.item())\n",
        "            # Clip gradients\n",
        "            nn.utils.clip_grad_norm_(self.encoder.parameters(), self.grad_clip)\n",
        "            nn.utils.clip_grad_norm_(self.decoder.parameters(), self.grad_clip)\n",
        "            # Optimize\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        # Randomly pick a sample from validation dataset to encoder\n",
        "        data, *_ = self.valid_dataset[np.random.choice(len(self.valid_dataset))]\n",
        "        # Add batch dimension and move it to device\n",
        "        data = data.unsqueeze(1).to(self.device)\n",
        "        # Sample\n",
        "        self.sampler.sample(data, self.temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RPY1E43sAnhb"
      },
      "outputs": [],
      "source": [
        "c = Configs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNJILEbgBjiP",
        "outputId": "d6e52c6c-6f58-4d3e-c626-be6b4736b6e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 700/700 [01:01<00:00, 11.30it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.38it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.39it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.32it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.28it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.35it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.35it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.37it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.37it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.24it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.26it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.28it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.24it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.17it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.24it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.22it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.25it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.10it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.33it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.19it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.23it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.09it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.04it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.17it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.18it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.24it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.24it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.23it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.16it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.15it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.19it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.30it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.32it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.27it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.32it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.11it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.25it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.32it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.32it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.34it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.34it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.30it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.38it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.20it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.19it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.27it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.31it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.11it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.25it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.39it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.29it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.19it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.29it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.20it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.39it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.40it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.43it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.42it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.29it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.30it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.31it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.30it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.28it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.30it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.26it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.28it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.22it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.09it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.08it/s]\n",
            "100%|██████████| 700/700 [01:00<00:00, 11.49it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.31it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.31it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.29it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.40it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.27it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.26it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.32it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.03it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.43it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.24it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.22it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.34it/s]\n",
            "100%|██████████| 700/700 [01:04<00:00, 10.93it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.30it/s]\n",
            "100%|██████████| 700/700 [01:01<00:00, 11.33it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.10it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.15it/s]\n",
            "100%|██████████| 700/700 [01:03<00:00, 11.11it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.18it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.25it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.12it/s]\n",
            "100%|██████████| 700/700 [01:02<00:00, 11.26it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 100\n",
        "\n",
        "for i in range(epochs):\n",
        "  for batch in tqdm(c.train_loader):\n",
        "    c.step(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "9g8gchrtEOOx",
        "outputId": "fc8af81a-ff57-4d17-d189-f91ed6cfa45f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGFCAYAAABg2vAPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABYZElEQVR4nO3ddXxd9f3H8dfV5MY9aZO6OxWsghQrVqC4DdsYjDEYsLH9mDBlLvjYGBS3AqMUKU6BQr2l7m2SNu5yc+38/rhJmpRK2t7kXHk/Hw8eSW7uvfl0a3Pe5yufr8UwDAMRERGJWVazCxARERFzKQyIiIjEOIUBERGRGKcwICIiEuMUBkRERGKcwoCIiEiMUxgQERGJcQoDIiIiMU5hQEREJMYpDIiIiMQ4hQEREZEYpzAgIiIS4xQGREREYpzCgIiISIxTGBAREYlxCgMiIiIxTmFAREQkxikMiIiIxDiFARERkRinMCAiIhLjFAZERERinMKAiIhIjFMYEBERiXEKAyIiIjFOYUCCPr8fnrsUSteaXYmIiPQwhQEJWvEcbHwHHjsJFj4EgYDZFYmISA9RGJCgjAHBj/4WePf/4KmZUFNobk0iItIjFAYkKHtY8GP6AHAkwPYF8MhkWPkiGIa5tYmISLdSGJCg7OHBjyn5cNNnUHA0tNTBazfCy9dAU5W59YmISLdRGJCgtpGB8vWQOQiuewdO/hlY7bD2f/Dw8bDpfXNrFBGRbqEwIEFZQ4MfmyqgsQJsdjjxR/Dt94PfayiBZy+EeXeCp9HcWkVEJKQUBiTImQhpfYOfl6/f83jv8fDdT+HYm4JfL/4PPDoNipb0fI0iItItFAZkj7Z1Ax3DAIDDBWf+Ea5+HZJ7Q9UWePx0+Og+8Ht7vEwREQkthQHZoz0MbNj39wedDN/7AkZfBIYfPvkDPH4aVGzquRpFRCTkFAZkj/2NDHTkSoeLHocLH4f4VNi1PDhtsOjf2oIoIhKhFAZkj4ONDHQ05iK4eSEMPBl8zfDWXfDMhVC3u3trFBGRkFMYkD2yW3cUNJR2ra9Aaj5c9Sqc+Sewx8OWD+Dh42D1q91bp4iIhJTCgOwRlwwpBcHPKzZ27TVWKxz7XfjuAuh1FLhr4JXrYM53oLmmmwoVEZFQUhiQzjo2Hzqk1w0N9iQ44cdgscLXLwXbGW/9OOQliohIaCkMSGeHsm5gbzYHTL8Hrp8PGQOhrhieOg/e+Sl4m0Nbp4iIhIzCgHR2uCMDHfU5Oni+waTrg19/+XDwaOTdK4+4PBERCT2FAensSEYGOnImwjl/hytehsScYLj493T49C/g9x15nSIiEjIKA9JZ246CumJw1x35+w09Hb73JYw4FwI++PA38MSZULX1yN9bRERCQmFAOnOlQ1Je8POu7ig4mMRMuORpOP9RcCZD0SJ4ZCosfVKNikREwoDCgHxTKNYN7M1igaMuD7Yz7jcVvI0w9zZ4/jJoKAvdzxERkUOmMCDf1JW2xIcrrS9cMxdO/y3YnLDxnWCjonVvhv5niYhIlygMyDe1jwwc4SLC/bFaYfKtcOPHkDsamirhxSvh9VtCs05BREQOicKAfFN3jgx0lDsKvvMhTLkdsMCKZ+DRKbD98+79uSIi0onCgHxTWxio2Qmexu79WfY4OO1XcN1bwSmEmp3w5Nnw3i/A19K9P1tERACFAdmXxExIyAp+HqodBQfTbzLc9DmMvwow4PN/BvsSlK7pmZ8vIhLDFAZk39pGB8q6eaqgo/gUOO8huPRZSMiE0tXBzoWf3w8Bf8/VISISYxQGZN9yemjdwL6MOCfYqGjoDPB74L2fw+yZUL2j52sREYkBCgOyb6FqS3y4knLg8hfg3PvBkQg7PoOHj4dF/4ZAwJyaRESilMKA7Ft3NB46VBYLTLwGbv4M+k4ONip66y6YfQ5UbjGvLhGRKKMwIPvWNjJQvd3844czBsK18+DMP7eOEnwOj0yBLx7UWgKRblbWVMbuht1mlyHdTGFA9i0xO3hOAQZUbDK7mmCjomNvDLYzHnAi+Jph/j3w3zPMm8oQiWJun5sHlz/IjDkzmPHqDH78yY/ZWN1Du4ukx1kMQyfFyH78dwbsXAiz/gNjLza7mj0MA5bNhnd/Bp76YFvjk34Ck28Dm93s6kQi3ieFn3Dfovsobij+xvdO7nMyN469kdFZo02oTLqLwoDs39zbgicLTrsLTvm52dV8U20RzL0dNr8X/LrXUcGtiXn6JSVyOIobivnDoj/wceHHAOQm5HL3MXfTN7kv//7638zfPh+D4CVjcu/JfGfMd5iUN8m8giVkFAZk/758BN75CQw/By571uxq9s0wYOUL8M7d4K4FqwNOuAum3gF2p9nViUQEj9/Dk2ue5N+r/o3b78ZusXP1yKu5adxNJDgS2p+3tXYrj3/9OPO2zsNvBNfrTMiZwI1jb2Ry78lYLBaz/ghyhBQGZP+2fAhPXwCZQ+DWJWZXc2D1JTDvTljfevphzig4/yHoPd7cukTC3MJdC/n9V79ne912ACblTuKeY+9hcPrg/b6mqL6IJ1Y/wWubX8Mb8AIwKnMU3xn7HU7uczJWi5ajRRqFAdm/ul3wtxFgscE9u4PnCIQzw4A1r8JbPwqehGixwZTb4MS7wRFvdnUiYaW0sZQ/L/kz725/F4DM+EzuOvouzh5wdpfv8EsbS5m9djYvb3gZt98NwOC0wdw6/lam953ebbVL6CkMyP4ZBvyhL7TUwc0LIXek2RV1TWNFMBCseTX4ddaw4FqCPkebW5dIGPAGvDy37jkeXvEwTb4mrBYrlw+/nFuOuoVkZ/JhvWeVu4pn1j7D8+ufp8HbgAULL5zzAiMzI+R3hmhroRyAxQI5rf+Yty8wt5ZDkZgFFz8Blz4DiTlQsQEePw3evQc8TWZXJ2KapaVLuWTuJfxlyV9o8jUxLnscL57zIj855ieHHQQAMuIz+MGEH/DuRe9yXK/jMDB4e9vbIaxcupvCgBzY6FnBj8ueCo4URJIR58ItX8G4ywEDFj4Ij06B7Z+bXZlIj6poruCez+7h2neuZXPNZtLi0vjV5F/x1JlPMTxjeMh+ToozhUuGXQIQ3HkQab8zYpjCgBzYmIvBFhc8QXDXMrOrOXQJGXDBo3DFy5DcG6q2wpNnwby7oKXB7OpEupU/4Of59c8z87WZvLHlDSxYuGjoRcw9fy6zhszqloV+U/On4rK72NW4i9UVq0P+/tI9FAbkwBIyYOTM4OfLnjK3liMx9HS45UuYcE3w68X/Dh58tOUjc+sS6Saryldx+bzL+f1Xv6feW8+IjBE8e9az/PL4X5IWn9ZtP9dld3FiwYkAzN8xv9t+joSWwoAcXNsF9OtXIvtuOj4VZt4PV78OaX2hdic8fT68cWuwR4FIFKhx13DvF/dy1VtXsa5qHcmOZO459h6eP/t5xmSP6ZEaTu9/OqCpgkiiMCAH139q8LAgTwOsfd3sao7coJODuyOOuTH49bKn4KHjYKPuYiRyBYwAczbO4dzXz2XOpjkYGMwcNJO5F8zlsuGXYbPaeqyWjlMFayrX9NjPlcOnMCAHZ7HA+KuDny+dbW4toRKXBGf9Ga59Kxh06nfBcxfDazdBU5XZ1YkckvVV67n67au5d+G91LTUMDhtME/OeJLfTf0dma7MHq/HZXdxQsEJAO19DCS8KQxI1xx1ZbCJT9EiKFtndjWh038K3PQ5HP99sFhh5fPw8HGw7k2zKxM5qHpPPfd9dR+Xvnkpq8pXkWBP4K5Jd/HSuS8xMXeiqbWd3k9TBZFEYUC6JjkXhp0Z/HzZ0+bWEmrOBDjjd3D9/GCDooZSePFKeO1m8HnMrk7kGwzDYO6WuZz72rk8t/45AkaAGf1n8Mb5b3DNqGtwWB1ml8i0gmmaKoggCgPSdRO+Ffy48nnwtZhbS3foczR891OYdmdwFGTlc/D8pZG9aFKizubqzVz/7vX832f/R6W7kv4p/XnstMf484l/Jjcx1+zy2nWcKpi/Xetxwp3CgHTd4FODe/Wbq/YcCBRtHPFwyi/gypfAkRg8rGn2ucEWxyImavI28dclf+XiuRezpHQJ8bZ4bptwG3NmzuH43sebXd4+tU8V7NBUQbhTGJCus9pg/FXBzyO550BXDD4VrpkLroxgs6X/ngHVO8yuSmKQYRjM3z6fma/P5Mk1T+IzfEzvM53Xz3+db4/5Nk5b+B7V3TZVUNxQzNrKtWaXIwegMCCHZvxVgAW2fgxV28yupnsVTIQb5kNqH6jcHAwEpZr7lJ6zo24HN71/E3d+cielTaXkJ+Xz0CkP8c/p/yQ/Kd/s8g7KZXcxLX8aoF0F4U5hQA5Ner/gPn2A5c+YW0tPyBoSDAQ5I6F+N/z3TNjxhdlVSZRz+9w8sPwBLvjfBXyx6wucVic3jbuJ1897vX0ePlK0NyDSVEFYUxiQQ9e2kHDFs+D3mVtLT0jpDde9BX2Ph5ZaePoCWP+W2VVJlKpsruT6d6/nsVWP4Q14mZI/hdfOe41bjrqFeHu82eUdsmn5miqIBAoDcuiGnQUJmcE75c3vmV1Nz3Clw9WvBf/sPndw62G0r5uQHre9djtXvXUVX1d8TYozhb+d9DceOeUR+qb0Nbu0w5bgSGBS7iQgeISyhCeFATl09rjWY4GJrQuiwwWXPB1cN2EEgmcaLPhr5B3tLGFpRdkKrn77aooaishPyueZs57htH6nYbFYzC7tiJU2lQKQnxz+6xxilcKAHJ62qYKN70LdbnNr6Uk2O8x8EKbeEfz6g1/DOz+BQMDcuiSivbfjPW549wZqWmoYnTmaZ856hgGpA8wuKySafc1sqdkCwKjMUSZXI/ujMCCHJ3sY9DkODH9w7UAssVjg1F/CjD8Ev/7qUXj12+pWKIflqTVPcefHd+IJeDip4CQeP+NxslxZZpcVMhuqNuA3/GS5sshNCJ+mSNKZwoAcvomtRxsvfzo274yPuxkufBysDlg9B567BFrqza5KIoQ/4OePi/7In5f8GQODS4ddyj9O/gcJjgSzSwuptlbEozJHRcWUR7RSGJDDN/I8iEuB6u2wfYHZ1ZhjzEVwxYvBboVbP1K3QukSt8/NnZ/cyTPrgttz75h4B/cce0+PHjPcU1ZXrAZgVJamCMKZwoAcPmcijLk4+PmyKDna+HAMPiXYrTAhE3Yth8dPV7dC2a8qdxU3zL+BD3Z+gMPq4E8n/InrRl8XtXfNHUcGJHwpDMiRaVtIuG4uNFWZW4uZCibC9e9Cal+o2hIMBCWrza5KwszOup1c/dbVrCpfRYozhcdOe4wzB5xpdlndpsHTwPba7YDCQLhTGJAj0/soyBsLfg+setHsaszVsVthQwk8cZa6FUq7FWUruOqtq9hZv5P8pHyePvNpJuVNMrusbrWuah0GBr0Se5HpyjS7HDkAhQE5cm0LCZfO1p77lF6duxU+dT6si9ITHqXLPtjxAd+e/22qW6oZmTmSZ856hoFpA80uq9utqdAUQaRQGJAjN/oisLugfB0ULzO7GvN17Fbob4GXrg4GJYlJz657lh9+/ENa/C2cUHACT5zxRFRtHTyQ1ZVaPBgpFAbkyLnSYPjZwc/XvGpqKWFj726Fc38An/5FIycxJGAE+NPiP/GHRX/AwODioRfzz5P/GXVbBw9EIwORQ2FAQmP0rODHNa/HZs+BfWnrVjjtzuDXH/4G3r5b//vEALfPzV2f3MXTa58G4PYJt/Pz436O3Wo3ubKeU+OuoaihCICRmSNNrkYORmFAQmPQKeBMhroiKFpsdjXhw2KBU34BM/4Y/HrRv2DODeBrMbcu6TbV7mq+M/87vLfjPRxWB3+c9kduGHND1G4d3J+2Ewr7JvclNS7V5GrkYBQGJDQc8TD8rODna14zt5ZwdNxNe7oVrnlV3QqjVGFdIVe/fTUryleQ7EzmX6f9i7MGnmV2WaZo7y+g9QIRQWFAQmfUBcGPa1/XUPi+dOpW+HGwW2FDudlVSYisKl/FVW9fxY66HfRK7MXTZz7N0XlHm12Wado7D2q9QERQGJDQGTQ92J64fjcUfmV2NeFp8ClwbYduhf89I9jOWSLahzs/5IZ3b6DKXcWIjBE8e9azDEobZHZZplLnwciiMCChY4/rsKtAUwX7lT8Rrp+vboVR4rl1z3H7R7fj9ruZmj+VJ2c8SXZCttllmaqiuYLSplIsWLR4MEIoDEhojWrdVbD2dQj4TS0lrGUNbu1WOAoaSoPdCrd/bnZVcggCRoC/Lvkr9y26DwODC4dcyAPTH4iprYP707alcGDqQP3vESEUBiS0Bp4E8anBC9zOhWZXE97auxVODnYrfPoCdSuMEC3+Fn786Y95cs2TAPxg/A/45fG/jKmtgweixYORR2FAQsvuhOHnBj/XVMHBudLg6ldh2NkduhU+aXZVcgC1LbXcOP9G3t3+Lnarnd9P/T3fGfudmNs6eCBaPBh5FAYk9Np3FfxPUwVd4XDBJU/B+KtbuxXeBp/8Wd0Kw1BhfSFXvXUVy8qWkexI5l+n/otzB51rdllhxTAMjQxEIIUBCb2BJwb78zeWww7Ng3eJzQ4zH4BpdwW//ui38PaPtUUzjKyuWM1Vb13F9rrt5CXmMfvM2RzT6xizywo7JY0lVLmrsFvsDEsfZnY50kUKAxJ6NgeM0FTBIbNY4JSfw5l/Cn696DGYc726FYaBjws/5vp3r6fKXcXwjOE8e9azDEkfYnZZYaltVGBw+mDi7fEmVyNdpTAg3aN9quAN8PvMrSXSHPvdDt0KX4NnL1a3QhO9uP5FbvvoNpp9zUzpPYUnZzxJTkKO2WWFLa0XiEwKA9I9+p8ArgxoqoDtC8yuJvKMuQiufCnYrXDbJ/DkOepW2MMCRoC/Lf0bv/3qtwSMALOGzOKBUx4g0ZFodmlhTesFIpPCgHQPmx1Gzgx+rqmCwzNoOlz7ZrBb4e4V8N/T1a2wh3j8Hn7y6U94YvUTAHz/qO9z7/H34rA6TK4svHVcPDg6c7TJ1cihUBiQ7tM2VbBuLvi95tYSqfIndOhWuLW1W+HXZlcV1WpbarnxvRt5e/vb2C12fjf1d3x33He1dbALCusLqffU47Q6GZw22Oxy5BAoDEj36TcVErKguQq2fWp2NZFrn90KPzO7qqhU3FDM1W9fzdLSpSQ5knjktEeYOWim2WVFjLZRgWEZw3DYNIoSSRQGpPvY7DDyvODna141t5ZI16lbYR28eiN43WZXFVXWVK7hynlXsq12G7kJucw+czbH9TrO7LIiihYPRi6FAele7VMFb4LPY24tka6tW+GYi+GSp8GhbVuh8mnRp1z3znVUuisZmj6UZ896lqHpQ80uK+Jo8WDkUhiQ7tVvMiTmgLsmuCpejozDBRf+Bwomml1J1Hh548vc+uGtNPuaOb7X8cyeMZvcxFyzy4o4/oCfdZXrAC0ejEQKA9K9rLYOUwXaVSDhI2AE+Oeyf/Lrhb8mYAQ4f/D5PHTqQyQ5k8wuLSJtr9tOk68Jl93FgNQBZpcjh0hhQLqfpgokzHj8Hn664Kf85+v/APC9cd/j15N/ra2DR6BtimBExghsVpvJ1cihUhiQ7tf3OEjKCx7Tu+VDs6uRGFfbUst33/sub217C7vFzm+m/Iabj7pZWwePUPviQa0XiEgKA9L9rDYYdX7wc00ViIl2NezimrevYUnpEhIdiTx06kOcP/h8s8uKCu2LB7WTICIpDEjPaJsq2PCWtsSJKdZWruXKt65kS+0WchJymD1jNpN7Tza7rKjgDXjZULUBgNFZWjwYiRQGpGcUHAPJvYN75DVVID1sQdECrn3nWiqaKxiSPoRnz3qWYRk6XjdUttRsocXfQrIjmT7JfcwuRw6DwoD0DKtVUwViijkb57RvHTy217HMnjGbvMQ8s8uKKmsqglMEI7NGYrXoshKJ9P+a9JxOUwXN5tYiUc8wDB5Y/gD3LrwXv+Fn5qCZPHLKIyQ7k80uLeqsrlTnwUinMCA9p+BoSO0DngbY/L7Z1UgU8/q93PPZPTy26jEAbhp3E7+d8lv1y+8mbSMDCgORS2FAeo7FogZE0u3qPHXc9P5NzN06F5vFxq8n/5pbjrpFWwe7SYu/hU3VmwAtHoxkCgPSs0bNCn7c8A54msytRaJOSWMJ17x9DYtKFpFgT+ChUx7igiEXmF1WVNtYtRGf4SM9Lp1eib3MLkcOk8KA9Kz8CZDaF7yNsPk9s6uRKLK+aj1XzruSzTWbyXZlM/vM2UzJn2J2WVGv4+FEGn2JXAoD0rMsFu0qkJD7vPhzrnn7GsqayxicNphnz3qW4RnDzS4rJujY4uigMCA9r21XwcZ3wdNobi0S8V7b9Bq3fHALTb4mjsk7htlnzqZXkoare0rbyIDWC0Q2hQHpeb3HQ3p/8DYFA4HIYTAMg4dWPMQvvvgFfsPPuQPP5dFTHyXFmWJ2aTGjydvE1tqtAIzMHGlyNXIkFAak51kse0YHNFUgh8Hr9/Kzz3/GoysfBeDGsTfyu6m/09bBHra+aj0BI0COK4echByzy5EjoDAg5mgLA5vmQ0uDubVIxLl34b28seUNbBYbvzz+l9w6/lYtXjNBx8WDEtkUBsQceWMhYyD43LDxHbOriRqGYdDQ4qOwqol1u+sorGqitsmLP2CYXVpIXTPqGnIScnhg+gNcNPQis8uJWVo8GD3sZhcgMaptqmDBX4NTBWP0C31fmj1+qps8VDV69nxs9FDV5G392Pp16/erG714/IF9vldyvJ2UeAcpLgcp8XZSXA5SXY7Wx4LfS3V1/n7bcxKdtrC68x6aPpS3Zr1FnC3O7FJi2trKtYAWD0YDhQExz6hZwTCw6T1w10F8dC/8avH5qWnydrigt13Ivfu54Htwe/d9YT+YOLuV5Hg7jS1+mr1+AOrdPurdPoprDv1cCKuF1pDQFhhag0W8g9SEDuGhNVjsCRnBj/EOa8jDhIKAueo8dWyv2w5o8WA0UBgQ8+SOgswhULkpOFUw9hKzK+oynz9AdZO39W687SLu/cbFfM9HLw0tvsP6WQ6bhfQEJxmJzj0fEx1kJDhJT9z7cScZCU5cTlv76z2+AHVuL3XNXurcPuqavdQ2e1sf81Hnbv26w/frWr9f2+zF6zcIGFDT5KWmyXtYfwanzdoeIJJbRx72jELsK0DYyUqKIz/NhdUaPiMSsse6ynUA5Cflkx6fbnI1cqQUBsQ8bVMFn/4pOFVgYhgIBAy2VzZS1djxDn0/F/dGD3Xuw7uw26wW0hMcpCfsuXAHL+iOb1zQ2z4/0iF6p91KVlIcWUmHfidtGAYtvkCHsLD/ALG/gBEwwOMPUNHgoaLBc0g/P95hZWBWEoNzgv8Naf3YLzMRp11Lnsyk9QLRRWFAzNUWBja/D+5aiE/t0R+/tbyBOcuKeG1ZMbtq3Yf0WosFUuIdNHl8jC1Ia72YOzpf5DvcvWckOEmOt0fUna7FYiHeYSPeYSM3Jf6QX28YBo0e/56wcAijE+X1Lbi9AdburmPt7rpO72u3WuiXmdAeEoJBIZmB2YkkOPVrrSdoJ0F00b8aMVfOCMgaBhUbYP1bcNTl3f4ja5u9zFu1mznLili6o7r98XiHldyU+L2G5B3fuKi7HDbW7a5j2c5qPtlQTrPHz1PXH0NinP457c1isZAUZycpzk7vNNchvdbnD1BY3czmsgY2lzWwqayeLa2fN3r8bClvZEt5I++uKe30uvw0V6dRhLb/0hKcofyjxbz2xYOZWjwYDfTbS8zVNlXwyR+CUwXdFAb8AYPPNlfwytIi5q8pocUXXJhntcCJQ7O5cGIBp47IJd5h2+fri2ua+XB9GS8s2skXWyrbXw/BELGhtJ4JfTVvGkp2m5UBWYkMyErktJG57Y8bhsHuWnd7SNhc3sDm0uDHqkYPxTXNFNc088nG8k7vl5UUx+CcxPZRhLaQkJMcF1Y7JSJBlbuK4oZiAEZkjjC5GgkFhQExX1sY2PIhNFeDK3QX1c1l9byytJjXlhdRWtfS/vjQ3CQunFDABePzydnH8Lc/YLB8ZzUfrC/jo/VlrC+p7/T9/DQX04fnMH1EDscPzNxviJDQs1gs9E5z0TvNxQlDszt9r6rR0z6K0B4WyhrYXeumoqGFioYWvtxa1ek1yfH2YDDIbp1uyE1icHYyBelavLg/baMC/VP6k+xMNrkaCQWFATFfznDIGQlla4NTBeOvPKK3q23y8saqXbyytIiVhTXtj6clODhvXG8unFjAmPzUb9wN1jR5+GRjOR+uL+OTjeWdVs5bLTCxXzrTh+cyfXgOQ3OTdDcZhjISnRwzIINjBmR0eryhxdc+xbCp9eOW8gZ2VDZS7/axfGcNy3fWdHpNnN3KwOzO0w1DtHgR6LB4UOsFoobCgISHURcEw8Ca1w47DCzdUc1/P9vGe2tL2xvv2KwWTh6WzUUTCzh5eA5x9s538KV1bl5dVsxH68tYsqOKjo36Ul0OThqWzfThOZw4NFtzzhEsKc7OuD5pjOuT1ulxt9fP9srGTqMIm8sa2FrRSIsvwLrddazba/GirW3xYnbrKEJOcCRhUE7sLF5sP6lQ6wWiRmz8zZXwN/J8+Oh3sPUjaKqChIyDvmRvW8obmPf1bgCG5yVz0cQCzjsqn+zkfW+p+9+KYn72+mrqO2wTHJabzPQROUwfnsP4PmnYbbF9Bxjt4h02huelMDyvc8Mrf8CgsKqpfRRhz9qEeho9fraWN7K1vJH5a/e9eHHvBYzRFiTXVgSnCTQyED0shmFEV9NyiVyPTIHS1TDzAZjwrUN+eUOLj3+8t5ELJuQzqvf+tyjWNnn5+f9W88bKXQCMyU/lkknBkYOC9ITDLl+in2EYlNS5O+xwaJ1yKGugsnH/PRSykpx7djZkJzEkNzliFy+WNZVxysunYLVYWXj5QhIc+jcTDTQyIOFj1AXBMLDmtcMKA0lxdn52zoHbon6xpYI7X1rJ7lo3NquF204ZwvdOGqQRAOkSi8VCr1QXvVJdTBuy78WLe2+F3FXrbm24VPWNxYsp8XZmjM7j0qP7MqFvWkQEgzUVwSmCQWmDFASiiMKAhI9RF8CHv4Gtn0BjJSRmhuytW3x+/vLuBv7z2TYMAwZkJfL3S4/iqL3mkEUO1/4WLza2+NhS3sCm1u2PbSMJ2ysbqXP7eGlJES8tKWJIThKXHt2HWRMKyEgM32mF1ZXqPBiNFAYkfGQOCh5tXLIK1s+FideG5G3Xl9Rx+wsr2rcHXnFsX3529oiYWewl5kqMszO2II2xBWmdHm/x+Vmxs4aXlxbx5qpdbCpr4Lfz1vHHd9Zz+qg8Lp3Uh6mDs8Jue6MWD0YnrRmQ8LLgb/DBr2DgSfCt/x3RWwUCBk98sZ0/vrMejy9AZqKTP144llM7NLARCQd1bi9zV+7ixcWFrCqqbX88P83FJZP6cPGkgkPu4NgdDMPghBdPoKalhufPfl5HF0cRhQEJL1Vb4f7xYLHCnRshKfvgr9mH3bXN3PXySj7fXAnAKcNz+MOFY/e7s0AkXKzZVctLiwt5bXlx+4FYltZOmZcd3Yfpw3NN63NQ3FDMjDkzsFvtfHXFVzht4TudIYdGYUDCz79OhN0r4Oy/wdE3HPLL31y1i3teW01tsxeXw8bPzhnBFcf0jYjFWSJt3F4/76wu4YXFOzstPMxKcnLhhAIuOboPg7KTerSm+dvnc+cndzIycyQvnvNij/5s6V6aNJXwM+0OcNfB8LMP6WV1bi/3/m8Nry4P9kwfV5DK3y89ioE9/AtTJBTiHTbOH5/P+ePz2V7RyEtLCnl5aRHl9S3869Ot/OvTrRzTP4NLju7D2WN64XJ2f0tsLR6MXhoZkKiwaFsVP3xxBcU1zVgt8P2TB3PrKUNwaMugRBGfP8BHG8p5cfFOPlxf1t4xMznOzsyjenPZ0X0ZnZ/SbaNg337323xV8hW/mvwrZg2Z1S0/Q8yhMCARzeML8Pf3N/LoJ1swDOibkcDfLx3HxH6H3sFQJJKU1LqZs6yIFxcXsrOqqf3xkb1SuOyYPpw3Lp/UBEfIfl7ACDDl+Sk0eBt45dxXGJYxLGTvLeZTGJCItbmsntteWMGaXcHe8ZdMKuAX544iKU6zXxI7AgGDL7dW8uKSQt5eXYKn9XjtOLuVM1sbGh03MOOIRwu2127n3NfPJc4Wx5dXfIndqn9n0URhQCKOYRg8tXAHv39rHS2+AOkJDu6bNZYZo/PMLk3EVDVNHl5fXswLiws7HbvdPzOBS47uw0UTCvZ5ZHdXzNs6j58s+AnjssfxzFnPhKpkCRMKAxJRyurc/OiVVXyysRyAE4Zm85eLxh72LziRaGQYBquKanlhcSFvrCim0eMHgicuTh+ew2VH9+HEodmH1Ib7j4v+yDPrnuGK4Vfw02N/2l2li0kUBiRivLO6hJ++uorqJi9xdiv/d9YIvnV8P20ZFDmAxhYf877ezYuLC1m6o7r98dyUOC6e2IdLJvWhb+bBzxi45u1rWFa2jN9P/T3nDjq3O0sWEygMSNgLBAzueX01zy/aCcCo3in887KjGJyTbHJlIpFlc1k9Ly4uZM6yYqo6nLI4eVAmlx7dhzNG5RHv+OYWRV/Ax+TnJ9Psa+Z/5/2PgWkDe7Js6QEKAxL2/vn+Jv7+/kYsFrjpxEH88NShpnVgE4kGHl+A99eV8sLiQhZsKqftKpDqcnDB+HwuPboPI3qltD9/U/UmZr0xiwR7AguvWIjVon9/0UbLQSWsfbyhjH98sBGAP144lksm9TG5IpHI57RbOWtML84a04ui6iZeXlLEy0sK2VXr5skvtvPkF9sZV5DKpUf35dxxvVhdEWw2NDJzpIJAlNLIgIStwqomzn3wM2qavFx+TF/umzXG7JJEopY/YPDZ5gpeXLyT99aW4vUHLw0uh42Bw99lp+99rhl5DXcdfZfJlUp30MiAhCW318/3nl1GTZOXsQWp/PLckWaXJBLVbFYLJw7N5sSh2VQ0tPDasmJeWLyTLeWNbKvfgM0Fr39lJc29lQvG55OZpEO/oolGBiQs/fTVr3l+0U7SEhy8eetUCtIPvtpZRELLMAy+2lbGjQtmYOCjYfOPMLyZOGwWTh+Zx6VH92Hq4CysVu3oiXQaGZCw8/KSQp5ftBOLBe6/bLyCgIhJLBYLKamVGPhIdqZw99kn8PKSIlYW1TLv693M+3o3+WkuLp5UwMWT+pCf5jK7ZDlMGhmQsLJmVy2zHv6CFl+AO04byg9OGWJ2SSIxbXP1Zp5Y8wROm5NfHv9LANbuquOlJYW8uqyIOrcPAIsFThiSzWVH9+GUEbna8RNhFAYkbNQ2eTnnwQUUVjVz8rBsHr/maA0/ioQxt9fPu2tKeGFRIQu3VrY/npno5Krj+vH96YN1cmiEUBiQsBAIGHz7qSV8uL6MgnQXb946lbQEp9lliUgXba9o5KUlhbyytIiy+hYAjh2QwcNXTtBiwwigMCBh4cEPN/GX+Rtx2q28evNkRuenml2SiBwGnz/Am6t287PXV9PQ4qN3ajz/unoSYwr0bzqcKQyI6RZsKudb/12EYcCfLhzLJUersZBIpNtcVs+NTy1la0UjcXYr980aw6wJBWaXJfuhMCCmKq5p5pz7F1Dd5OWyo/vwhwvHml2SiIRIndvL7S+s4MP1ZQBcP2UA/3fW8EM6LVF6hsKAmKbF5+eSRxeysqiWMfmpvHzT8fs8JEVEIlcgYPCP9zdy/4ebATh+YCYPXjFe6wjCjMKAmOae177m2a92kuoKNhbqk6F+AiLR6p3VJdz50goaPX7y01z86+qJWhsURjRWI6aYs7SIZ78KNhb6x2VHKQiIRLkZo/N4/ZYp9M9MoLimmYse/YL/rSg2uyxppZEB6XHrdtdxwcOf4/YGuO2UIfzwtKFmlyQiPaS22cvtLyznow3lAHx76gB+cqbWEZhNYUB6VG2zl5kPfsaOyiZOHJrNE9eqsZBIrPEHDP7+3kYe/Ci4jmDK4EweuHwCGYnqLWIWhQHpMYGAwY1PL+X9daXkpwUbC6XrH79IzHr7693c+fJKmjx+CtKD6whG9dY6AjNoXEZ6zCOfbOH9daU47VYevWqigoBIjDtzTC9e+94U+mUmUFTdzIWPaB2BWTQyID3i880VXP34VwQM+MOsMVx2TF+zSxKRMFHb5OUHLyznk43BdQQ3njCQH58xTOsIepDCgHS73bXNnHP/Z1Q2erhkUgF/umic2SWJSJjxBwz+On8DD3+8BYCpg7N44PLxGkHsIQoD0q08vgCXPraQ5TtrGNU7hTk3T1ZjIRHZr3mrdnPXyytp9vrpk+HiX1dNYmTvFLPLinoag5Fu9dt5a1m+s4aUeDuPXDlRQUBEDujssb147ZbJ9M1IoLAquI5g7spdZpcV9RQGpNu8vryYpxbuAIKNhfpmqrGQiBzc8LwU3vj+FKYNyaLZ6+fW55dz39vr8Ac0kN1dNE0g3WJ9SR3nPxRsLPSD6YO54/RhZpckIhHGHzD487sbePST4DqCaUOC6wjSErSOINQUBiTkPL4AM/7xKVsrGpk2JIsnrzsGmxoLichhmrtyFz9+ZRXNXj99MxJ47FsTGZ6ndQShpGkCCbn1JXVsrWgkJd7OPy8bryAgIkfk3HG9mXPzZPpkuNhZ1cQFD33BvFW7zS4rqigMSMiV1rUAMCArUe1FRSQkRvZO4Y1bpravI7jluWX88Z31WkcQIgoDEnKldW4AspPjTa5ERKJJeqKTJ649mhtPGAjAIx9v4bonF1Pb5DW5ssinMCAhV1YfHBnISYkzuRIRiTZ2m5X/O2sE/7zsKOIdVj7dWM7Mhz5jQ0m92aVFNIUBCbny+uDIQK5GBkSkm5x3VD5zbp5MQbqLHZVNXPDw57z9tdYRHC6FAQm5tjUDGhkQke40qncqb3x/KpMHZdLk8XPzs8v487taR3A4FAYk5MpaRwZykhUGRKR7ZSQ6eer6Y/j21AEAPPTRFm6YvZjaZq0jOBQKAxJyZa0jA7kpmiYQke5nt1n52Tkj+edlRxFnt/LxhnLOe/AzNpZqHUFXKQxISPkDBhUNrdMEGhkQkR7Uto4gP83F9somLnjoc95ZrXUEXaEwICFV2dBCwACrBTKTFAZEpGeNzk9l7q1TOX5gJo0ePzc9s4y/zt9AQOsIDkhhQEKqbVthZlKcOg+KiCkyEp08fcMxXD8luI7ggQ838+2nlmgdwQEoDEhItS0ezNVOAhExkd1m5RfnjuRvl4wjzm7lw/VlnP/Q52wu0zqCfVEYkJBqWzyYox4DIhIGZk0oYM7Nk+mdGs+2ikbOf+gL3l1TYnZZYUdhQEKqvceAFg+KSJgYnZ/KG7dO5dgBGTS0+Pju00v523sbtY6gA4UBCSn1GBCRcJSVFMcz3z6Wayf3B+D+DzZx49NLqHNrHQEoDEiI7TmXQNMEIhJeHDYr984cxV8vHofTbuX9dW3rCBrMLs10YRMGWpp9LHt3B5uWlFKyrZamOg+GoSGcSFNWp5EBEQlvF04s4JWbjqdXajxbyxs5/6HPeW9tqdllmcpudgFtakqbWPjalk6P2R1WkjPjSc6MJyXT9Y3PXckOLBZtXwsnGhkQkUgwtiCNN74/lVueW8aibVV856kl3H7qEH4wfQjWGNwWbTHC5Pa7clcDy9/dSV1lM/WVbhpqWuAgle0JCy5SWoNCW1hIyYonPklhoScFAgZDf/Y2voDBFz+ZTu80l9kliYgckNcf4LdvrmX2wh0AnDoil79fOo7keIfJlfWssAkDe/P7AjRUu6mrdFPf+l9bUOhyWHBa9xkUgh8VFkKtsqGFib99H4BNvzsThy1sZqFERA7o5SWF3PP6ajy+AIOyE3nsW5MYlJ1kdlk9JmzDwMH4fQHqq/YdFOoq3TTWHlpYSGkdYUjOjCclKxgc4hMVFg7F2l11nHX/AjITnSz9+WlmlyMickhWFtbw3aeXUlLnJjnOzuPXHs0xAzLMLqtHRGwYOBi/N0B9deewUFfR9nUzjbWeg76HPc7WPqqQktE5LKRkuohLtCssdPDxhjKufWIxw/OSeef2E8wuR0TkkJXXt3DLs8tYtL2KrCQn834wLSZOYA2bBYShZnNYSctJIC0nYZ/f93v3jCy0jSrsmZIIhgVfi5+qXY1U7Wrc53s44mztUw5tQSE1y0VWnySSM+NjLiho8aCIRLrs5DhmX38MFzz8OetL6rn1ueU8951jsUf5tGfUhoGDsTmspOUmkJa777Dg8/ppqGrZZ1Coq3TTVOvBe4CwEJdgJ6tPMtl9k8num0R2n2TSchKwRPEq1fLWMJCrbYUiEsFcThsPXzmBmQ9+zqLtVfx5/gZ+euYIs8vqVjEbBg7G7rAdVlioKW2iclcDLU0+ijdUU7yhuv01jjgbWX2SWgNCMtl9kknPS8AaJYmztK3HgA4pEpEINzA7iT9dNJbvPbuMf32ylYl90zl9VJ7ZZXUbhYHDdKCw4PcFqNrVSHlhPeU7g/9VFjXgbfGze3MtuzfXtj/X5rCSVdA5IGT0TsRmj7yAoEOKRCSanDWmF9dPGcB/P9/GnS+vZF5eCn0z932DGOkUBrqBzW5tv7gzJfhYwB+gurSJip31lO9soGxnHRWFwYBQuq2O0m117a+32ixk5ieR3TqKkNU3maz8JOxOm0l/oq7RuQQiEm1+cuZwVhRWs2xnDTc/u5Q5N08m3hHev4sPR9TuJogERsCgtry5ffSgbSShpcn3jedarBYyeiWQ3ScYDrL7JpNVkIQzPnzy3JQ/fEhxTTNzbp7MxH7pZpcjIhISu2qaOeeBz6hq9HD5MX24b9ZYs0sKOYWBMGMYBvWV7mAw2LEnIDTX7+NkLQuk5SS0Ty9k90smu08ScQk93znLMAyG/ewdPP4AC358Mn0yonMoTURi06cby7nmiUUYBvzl4nFcNLHA7JJCSmEgAhiGQWONp9MahIrCehqqW/b5/JSs+E5rELL7JuNKdnZrjTVNHo769XsArP/NjKgcRhOR2PaP9zfyj/c3Ee+w8votUxiel2J2SSGjMBDBmur2BISK1mmGugr3Pp+blB73jYCQkOoMWS+EjaX1nP73T0lLcLDiF6eH5D1FRMJJIGBw7ZOL+XRjOQOyEnnj+1Oi5gwDhYEo4270UlEYXKRYvrOO8sIGakqb9vlcV4qzNRgkkdM3hay+SSRnHF6zpAWbyrn68UUMzU1i/g9PPNI/hohIWKpq9HD2/QvYXevm7DG9ePCK8VHRYC58Vp9JSMQnOigYnkHB8D39tD3NPiqKGjotUqze3UhznYedayrZuaay/blxifb2kYO2UYTUbNdBmyVpW6GIxIKMRCcPXTmBSx5dyLyvdzPpi3SumzLA7LKOmMJADHC67PQekkbvIWntj3k9fiqLGzotUqza1UhLo4+i9dUUre/QLCne9o2AkJaX0OnM7/ZWxNpWKCJRbkLfdO45ewS/mruW381bx9iCtIjfQaUwEKMcTht5A1LJG5Da/pjfG6Bqd2OnrY4VRQ143X52baph16aa9ufana3Nklq3OlYU1WM1dC6BiMSGayf3Z8n2auZ9vZvvP7eMeT+YRkZi9y7U7k5aMyAHFPAHqC5p6twLobABX4v/G8/1YeDIiGPEqKz2kYTM/ETs2lkgIlGo3u3lvAc/Z2tFI9OGZPHkdcdgi9DzZxQG5JAFAga1ZU2t0wvBhYo7NtVgD3zzuVarhfReia09EPY0S3LEKSCISORbX1LH+Q99jtsb4IenDuW2U4eYXdJhURiQkDjxTx9SU+7mj9OHk9piULGznrKd9bgb9t0sKT03gaw+ySSmxeFKcuBKduBKchLf+tGV7MARZ4uKVboiEt3mLC3izpdXYrHA7OuO4YSh2WaXdMgUBuSIGYbBiF+8g9sb4JMfnUS/zMT2xxtrWihrXaRY0TrV0Fjr6dL72uxWXMkO4pMcuJKdwdDQHhg6PJbsJD7JQVyCXeFBREzx01dX8fyiQtITHMz7wTR6p7nMLumQKAzIEatzexl773wA1v16Bq6DHKjUWNtCRWEDlcUNNNV7cNd7aW7w0Nz60V3vxefdx5zDQVitltbg4CC+dXTB1f7xm4/FJTo67YgQETlcbq+fCx/5gjW76pjQN40XbjweZwSdPqvdBHLE2noMJMfZDxoEABJT40hMjaPf6Mz9Psfb4qe53kNzg5fmeg/uBm97WGhu8OJu+17r5x63n0DAoKnOQ1OdB2g8eOGWYF+GjqMM8e2jDXuNQiQFRx8i8WhpEel+8Q4bD185gXMe+IxlO2t45OMtEbV+QGFAjljb0cXZKaHrMeCIs+GIc5GS1bWhNr830BoOvjnSEAwMnR9rafSBAe4GL+4GL9Ul++7SuDeny94eFvY5+tBh6sKV5Aj7Y6dFJHT6ZSbyq5mjuOOllTy3aAffnz44YnYXKAzIEWsbGcg1sfugzWElKT2OpPSuBZKAP4C70beP0Ye2r724Gzp/zzCC3Rw9zT5qy5u79HPscbbWkYU9AWHS2QNIzY6s+UQR6Zqzx/bi12+upbSuhc82V3BihCwmVBiQI1bVGFwQmJEUOQ03rDYrCSlOElK6VrMRMGhp8nUecWgLDPXe9tDQcQoj4Dfwtfipb/FTX7nnAKnxZ/Trrj+WiJgszm7jvHG9mb1wB68sLVIYkNjhaJ1H9/ujdy2qpXVxYnySg/S8gz/fMAw8bv8+Rhw8XR69iBVta5i1E0SixYUTC5i9cAfz15RQ2+wl1RX+JxtqNZQcMVdrh8Fm7ze7EsYqi8VCnMtOWk4CeQNTGTAum5FTejNxRn+c8crgbaqefZZt552Pe+VKs0sRCZkx+akMzU2ixRdg3qrdZpfTJQoDcsTaw4BHYUC6JtDYSPXzz1P6m9/SsnEjtW+8YXZJIiFjsVi4aGIBAK8sLTS5mq5RGJAj5nIG/xppZEAOxrN9O6X33cemk06m5Fe/bn+8bt5bGJ6uNaMSiQTnH5WPzWph2c4atpY3mF3OQSkMyBFzOYLD3goDsi9GIEDDp5+y88Yb2TLjTKpmP0Wgvh5nv37k3H03FpcLf20tDQsWmF2qSMjkpMRzwpAsAOYsKzK5moNTGJAj1tZoSNMEsjfD52P7ZZdTeON3afx0AVgsJJ14In3+/W8Gvv0WmdddS/qllwJQ+z9NFUh0uWhiHwBeXVaMPxDeC6wVBuSIta0ZcGtkQPbSsmkT7lWrsDgcZFx7LYPefYc+/3qUpGlTsViDv35Szz8PgIaPPsJfW2tmuSIhdcqIHFJdDnbXuvliS4XZ5RyQwoAcMe0mkP1pXrkKgISjJ5H7k7tx9u37jefEDx9O3NChGF4vde+829MlinSbeIeNc8f1AoInG4YzhQE5YvEdFhDq3CvpqHlVMAzEjxl7wOelnjcTQLsKJOq0TRW8s6aEOvc+jnQPEwoDcsTaRgYMA1p8h37aoEQv99fBMOAad+AwkHLOOWCx0Lx0KZ6i8L6DEjkU4wpSGZyThNsb4K0w7jmgMCBHrC0MgBYRyh7+hgZaNm8BwDVmzAGf68jNJfH44wCNDkh06dhzIJx3FSgMyBGz26w4beo1IJ25V68Gw8Deuxf27IP3Z0+ZGZwqqPvfG5pukqhywfh8rBZYvL2a7RVdOF7dBAoDEhLxDoUB6ax51dcAuMaO69LzU047DYvLhWfHDtytaw1EokFuSjzThgQDcbiODigMSEio14Dszb16NQDxw4Z26fnWxESSTz0VgNo35nZbXSJmaJsqeHVZMYEw7DmgMCAhoV4DsjdHQfCXX/WLL+Grru7SaxImTQLAq0WEEmVOG5lLcryd4ppmvtxaaXY536AwICERr14Dspes730PZ//++EpK2P2Tn2IEDr7TxN8aGmyZmd1dnkiPCvYc6A3AK2HYc0BhQEKibZqgSdME0sqWlEj+P/6Oxemk4ZNPqHriyYO+xlcZvGOyKwxIFGqbKnh7dQkNLT6Tq+lMYUBCIsGpaQL5pvjhw8n9v58CUPb3v9O8YsUBn++vDLZstWcpDEj0Gd8njYFZiTR7/bz1dXj1HFAYkJBob0mskQHZS9qll5J85gzw+Si64w78NTX7fa6vIjgyYMvM6qHqRHqOxWLhwtbRgXCbKlAYkJDQmgHZH4vFQq/f/AZH3774du1m1//ds98+Au3TBBoZkCg1a0I+Fgss2lbFzsoms8tppzAgIaHDiuRAbElJ5P/9b1gcDho+/JDqp57a5/P8Fa3TBFozIFGqV6qLqYODI1/h1HNAYUBCom0BoVvTBLIfrlGjyLn7bgBK//JXmr/+GsMwCDQ24ikspGnZ8vYjjG1ZmiaQ6NWxPXG49Bywm12ARAeNDEhXpF95BU1ffUX9e++x4+pvAWC43Z2f5HBgS001oTqRnnH6yDyS4+wUVTfz1bYqjh9k/kiYRgYkJNrWDGhroRyIxWKh1+9+i6NvXwy3uz0IWOLicPTuTfzo0eTccQcWq341SfRyOW2cM64XED4LCTUyICHRtrVQIwNyMLaUFAa8/BKeHTuwZWRgT0/HkpCAxWIxuzSRHnPhhAKeX1TI26t38+vzRpEYZ+7lWPFbQsKlPgNyCGypqbjGjsVZUIA1MVFBQGLOxH7p9M9MoMnj5+3VJWaXozAgoRGvPgMiIl1msVjaFxK+srTQ5GoUBiREtIDw8BmGQUN1FcXr11JfVWF2OSLSQy6YUIDFAl9uraKwytyeA1ozICGxJwwc/DCaWORtcVNXXkZN6W5qS0uoKSuhtrSE2rJSastK8XlaADjl+ps56oyzTa5WRHpCfpqLyYMy+XxzJa8uK+a2U4eYVovCgIRE25qBZk94Hb7RU4xAgMaa6g4X+ZLWi34ptWUlNFZXHfD1FouV5Kxs0Ny5SEy5aGIBn2+uZM6yIn5wymDT1s8oDEhIxEI7Ym+Lu/VOvqTT3X1NaQl1ZaX4vJ4Dvt7pSiAttxepubmk5uSRlpvX+rEXyVnZ2Oz65ygSa84YlUdS3Bp2VjWxeHs1xwzIMKUO/faRkGjfWuiJ3GmC9rv70t3UlpVS0+EOv7ashMaa6gO+vu3uPi03l9TcXp0u+Km5ecQnJmnVvIh0kuC0c/LwHOau3MVXWysVBiSyta0ZCPethV63m9qy1uH7tot9Wdfv7uMSEknNzSOt9QLfdqFPy8nT3b2IHJbE1pspM+8V9JtLQsLVoemQYRim3QEbgQANNVXti/Pa7u7bFu411dYc8PUWq5WUrOz2i3zb3X1a651+fFJSz/xBRCRm+FvPJ7BazUsDCgMSEm1rBvwBA6/fwGnvvr/UXre700K9TsP55aX4vd4Dvj4uMbH94t5+l9/6eXJmlu7uRaRH+VuP9LaZODSg33oSEm3TBBAcHXDaD7+FhREI0FBdtWeR3l4L9rp0d5+dE7yr3+sOX3f3IhJu2k4utGlkQCKdw2bBZrXgDxg0e/ykuhwHfL7H3Rxcmb/XMH5NWSl1Xbi7j09M6nyR7/B5cmY2VpvtgK8XEQkX/tZTjK0aGZBIZ7FYSHDYqG/x0djcQp23nqaaahprq2msrqa+snzPcH5Z6UHv7q02GylZOa0X+c5b8XR3LyLRRCMDEjEMw8Dd2EBjdRWNNdXBC31NNQ0dPj9/WxFx3kZev+2RLr1nfFJyh3n7XN3di0hM8gWCW7K1gFBM4/N4aGy9mLfdxbdf7GurWy/+NTTWVBPwH7i7YFqHz602Gwlp6SSmppOYnk5SekZrw522u/tc4hN1dy8i4m9tz6IFhBJSRiBAc0N9+1383v81dbj4tzQ2HtJ7xyclk5iWTmJaGgmp6SSmZ7R+nc5vPihkQx387doTOHF0XyxWnYMlInIwgbbdBCb+ylQYiCBet3uvu/g9d+1NtdU0VFe13tHXYAS63gnQ5nAEL+itd/GJaenBC31a28U+rf0xu2P/CwObVn1OlbsGn92lICAi0kXtfQY0MhC7AgE/zXV1ey7k+7qTr62moboar7v5kN7blZyy5849NY2EtOBwffvwfVrw4h+XkBiSJkEuRzAARPP5BCIiobZnZEBhIKoYhoGnuXkfc+/VwTv5Dl8319VhGF2/i7c744J37x3u4hNT09sv9Ilp6SSkpZGQktbjzXPajzH2KAyIiHSVX7sJIovf56OprqZ9kV3bxb5hr3n4xppqfC0tXX5fi8VKQmpq8I69/a49rcNdfdvFPh1HvCtsD7tJcAb/OmlkQESk6xQGwoSnuYn6qsrgRb7DXXvb/Hv7XXx93SG9r9Pl2jP/3jb33mkuPvi5KzklKrbRZSY5AViyo5prJvc3txgRkQgRUDvi7uX1tNBYXU1DdSWN1VU0VFXRUF1JQ1Xr163/HcpcvMVqJTF1z117Qoe598SOc/Fp6Tji47vxTxd+LpnUh6cW7mDeql3ccdpQBmQlml2SiEjY00FFh8nv89FUWxO8sFdX7bm4t17s2y707ob6Lr+n05VAYnoGSR1W0Cd0uLC3XfxdSclaKb8fo/NTmT48hw/Xl/HIx5v500XjzC5JRCTstU8TaGQgyAgEaK6va71jD97BN1RVtV7c91z4m+pqoXVY5WDsDidJGZnBC316BkkZGSSmZ7Z+HvyYmJ6BM97VzX+62HDLyYP5cH0Zry4r5rZTh5Kfpv9dRUQOxB/ruwnWf/EpGxd+FrzoV1fRWFNFwN+1xWdWm43EtIz2i3nbhb3ThT89k7jE0Gybk66Z2C+d4wdmsnBrJY99soVfnTfa7JJERMJaWwfCmJ0mqN5VzKZFX3R+0GIhISV1z8U9vfXinhH8vO1xV3KKhuvD1PenD2bh1kqeX1zILdMHk5McW2snREQORSDWpwkGjJ8UbIyTsedOPiG15/fHS2hNHpTJ+L5pLN9Zw+MLtvHTs0aYXVLIBVp8tGyro2VzDakz+mOxK5iKyOFpmyYw8/7W1Ktu3qAh5A0aYmYJ0g0sFgvfP3kwN8xewjNf7uCmEweRnug0u6wjYvgNPEX1tGyuwb2pGs/OemhN866RGcQNTDO3QBGJWDE/MiDRa/rwHEb0SmHd7jqe+GI7d5w21OySDolhGPgqmlsv/jW0bKnBaOm8nsWWEU/84DSsCfs/r0FE5GBifgGhRK+20YFbnlvGk59v4zvTBpAcH94XTX+Dh5YtrRf/zTX4azp3kbS47MQPTiNucBrxg9OwZ2qnhIgcOXUglKg2Y3QeA7MT2VreyDNf7uTmkwaZXVInhtdPy/a64MV/UzXe3Xsd52yzENcvhbgh6cQPTsORn4TFxH+sIhKdAgoDEs1sVgvfO2kwd728ksc/28q1k/vjcprXdtkIGHh3NeDeHLzzb9leC77O/SoceYnEDUkjfkg6zv4pWE2sV0Rig09HGEu0O++o3vzj/Y0UVTfzwuKdXDdlQI/+fF+VG/fm6uDFf3MNgSZfp+/bUpzBO/8hacQNSsOWHNkLHUUk8ugIY4l6DpuVm04cxM9eX81jn27lymP74ezGbXiBJi8tW2txbwoGAF+lu9P3LXE24gamBuf+h6Rjzw7fUyBFJDZozYDEhIsmFnD/B5vYXevm1WVFXHZM35C9t+EL4NkZnPd3b67BW1QPHUf+reDskxK88x+chrNPMhabegKISPjwa5pAYkG8w8aNJwzkt/PW8cgnW7hoYgH2w7wgG4aBr7SpdcV/NS1bazG8gU7PsWe7iB+STtzgNOIGpmKN119zEQlfrVlAIwMS/a44ti8PfbSZHZVNvLlqN+ePz+/ya/21Le2L/tybqwnUezt935rkaN3ul07ckDTsqXGhLl9EpNvo1EKJGQlOOzdMHcBf5m/koY82M3Nc7/0eyhFo8dGytZaW1qF/X1lTp+9bHFacA/bM+ztyE7TlT0QiVsy3I5bYcvXx/fnXJ1vZVNbA/LWlzBidBwS3/HmK6mnZWI17c02nVr8AWMCRn9R+5x/XL0VnAYhI1FCfAYkpqS4H10zuz4MfbeahjzZzxqhcWrbWUvv2NrxFDZ2ea8uIb1/0Fz9ILX9FJHq1tyPWNIHEiuunDuDxz7bRUFzPxoeWk1gU7PpncVqJH5ahVr8iElMCAQNDCwgl1qT6DR7KyGBQqRtbUSNYLSQem0fKKX2xJanhj4jElrZRAVAYkBgQcPuo/7SIhgXFDPUGAAsf4WXkRcOZOKG32eWJiJjiPwu2AZDotJHgNO+SrDAg3crwB2hcVELd+zsJNAa3BDr7pfBCYoC/rd3FtOWFPK0wICIxaOmOKv4yfwMAPz9nZLd2Zz0YhQHpFoZh4F5TSe072/FVNANgz3KROqM/8aMyuaC6mX+u382CTRWsLKxhXJ80cwsWEelB1Y0ebn1uOf6Awcxxvbn06D6m1qP9WRJyLTvqKH9kJZXPrMNX0Yw1yUHa+YPI/eEEXKOzsFgs9MlI4LyjgiMCv3lzLW6v3+SqRUR6hmEY/OiVleyqddM/M4HfXTDa9DNSLIZhGAd/msjBecubqHtnO81rKoFgc6Ckafkkn1iANe6bg1DbKho55/4FNHr8TB2cxX+umUS8Q0cGi0h0+8+Crfx23jqcNiuvfm8yo/NTzS5JYUCOnL/eQ90HO2lctBuCawNJPDqPlFP7Yks5cGvgxduruOa/i2jy+Jk2JIt/f0uBQESi14rCGi5+9Au8foNfnzeKbx3f3+ySAIUBOQIBj5+GBcXUf1KE4QkO88cPzyD1zP44chO7/D6LtlVx7RPBQHDC0Gweu3qiAoGIRJ3aZi9n37+Aoupmzhydx8NXTjB9eqCNwoAcMsNv0LS0lNr3dhCo9wDgKEgi9cwBxA9KO6z3/GprJdc+sZhmr58Th2bzLwUCEYkihmHwvWeX8fbqEvpkuHjz1mmkusKns6rCgHSZYRi411dR+/b29sODbBnxpJ7RH9eYrCM+LOjLrZVc1xoIThoWDARxdgUCEYl8Ty/czs//twaHzcIrN00Oux1UCgPSJZ7Cemrf3kbL1loArAl2kqf3Jem4XiE9NGjhlkque3IRbm+A6cNzeOSqCQoEIhLRVhfXMuvhL/D4A/zs7BF8e9pAs0v6BoUBOSBfZTO183fQvLI8+IDdQvKUfJJP6oPV1T1tKr7YXMH1sxfj9gY4ZXgODysQiEiEamjxcc79C9he2cSpI3L597cmhs06gY4UBmSf/I1e6j/cScOXu8FvgAUSxueQcno/7Gnx3f7zP99cwfVPLqbFF+DUETk8fOVEU7tziYgcKsMwuO2FFbyxche9U+N567ZppCWE5xksCgPSieH10/DFLuo+KsRwB3cIxA1JI/XMATh7J/VoLZ9tquCG2W2BIJeHr5ygQCAiEeOFRTv5yatfY7NaeOm7xzGxX4bZJe2XwoB00vDFLmre2AKAo1dicIfA0HTT6lmwqZwbZi/B4wtw2shcHrpCgUBEwt/6kjrOe/BzWnwB7p4xnJtPGmR2SQekMCCdGN4A5Y9/TeLReSSMzzniHQKh8OnGcr79VDAQnDEqlwevmIDDpkAgIuGpyePj3Ac+Y0t5IycOzeaJa4/GGga/Sw9EYUAiwicby/lOayCYMSqPB64Yr0AgImHpzpdWMmdZEbkpcbz1g2lkJh24E2s40G9TiQhtjYicNivvrCnhB88vx+sPmF2WiEgnrywtYs6yIqwW+Odl4yMiCIDCgESQk4fltAeCt1eXcPsLK/ApEIhImNhcVs/PX18NwO2nDuW4gZkmV9R1CgMSUU4ensOjV0/AYbMw7+vd3PaiAoGImM/t9XPLs8tp9vqZPCiTW04ebHZJh0RhQCLO9OG5PHLlxGAgWLWbH760UoFAREz1q7lr2VBaT1aSk39cdhS2MF8wuDeFAYlIp47M5eHWQDB35S7uUCAQEZO8sXIXzy/aicUC/7h0PDnJ3d+YLdQUBiRitfUdsFstvLFyF3e+vBJ/QJtjRKTnbKto5KdzVgHw/ZMHM3VIlskVHR6FAYlop4/K48HWQPC/Fbu4S4FARHqI2+vn+88to9Hj55j+Gdx2yhCzSzpsCgMS8WaMzuPBK8Zjt1p4bXkxP1IgEJEecN9b61izq470BAf3Xz4eewT3PoncykU6mDG6Fw9cPh6b1cKry4v58SurFAhEpNu8/fVuZi/cAcDfLj2KvNTIWyfQkcKARI0zx/Ti/suCgWDOsiLunrOKgAKBiITY8p3V/Lh1ncB3TxzIycNyTK7oyHXPgfQiJjl7bC8MgseGvrI02AXsD7PGhn1fcBEJf4GAwaOfbuFv8zfiCxhM6JvGXacPM7uskFAYkKhzztjeGAbc9sJyXlpShAUL980ao0AgIoettM7ND19cwRdbKgE4Z2wvfj9rTNSckaIwIFHp3HG9MYDbX1jOi0sKsVjg9xcoEIjIoftgXSl3vbyS6iYvLoeNX503iosnFmCxRM/vE4UBiVozx/XGMAx++OIKXlhciMVi4Xfnj1YgEJEucXv9/OHt9Tz5xXYARvZK4YErxjMoO8ncwrqBwoBEtfOOyscw4I6XVvD8op1YLfCb8xQIROTANpfV8/3nlrO+pB6A66cM4O4zhxFnt5lcWfdQGJCod/74fAwM7nhpJc9+FWwZ+pvzRkfVEJ+IhIZhGLy4uJB7567B7Q2QmejkLxeP4+Thkb9j4EAUBiQmXDC+gEAA7nplJc98uROrxcKvZo5SIBCRdrXNXv7v1a+Z9/VuAKYOzuJvl4wjJyWyewh0hcKAxIwLJxZgAD96ZSVPLdyBBbhXgUBEgCXbq7jthRUU1zRjt1r40RnD+M60gTEzpagwIDHlookFBAyDu+esau8e9rNzRkbN9iAROTT+gMFDH23mH+9vJGBAv8wE7r9sPOP6pJldWo+yGIahFm0Sc15aXNjeQWxobhL3zhzF5EGRedqYiByeXTXN3P7iChZtqwLggvH5/Pq8USTHO0yurOcpDEjMmrtyF7/432qqm7xAsInIPWePoFeqy+TKRKS7vbO6hLvnrKK22Uui08Zvzh/NrAkFZpdlGoUBiWk1TR7+On8jz361g4ABCU4bt04fwg1TB+C0a+pAJNq4vX5+O28tz3y5E4CxBancf9l4+mclmlyZuRQGRIDVxbX88o01LN1RDcDArETunTmKE4Zmm1yZiITKhpJ6bn1+GRtLG4DgIUN3njZMwR+FAZF2hmHw6rJi7nt7PRUNLQCcMSqXn509kj4ZCSZXJyKHyzAMnvlqJ799cy0tvgBZSXH87ZJxCvsdKAyI7KXO7eUf721i9sLt+AMGcXYrt5w8mBtPGEi8Izq7j4lEq+pGD3fPWcX8taUAnDg0m79eMo6spDiTKwsvCgMi+7GhpJ5f/G81X7WuNO6bkcAvzx3JKSNyTa5MRLriy62V3P7CCkrq3DhsFu6eMZzrpwyImd4Bh0JhQOQADMNg7qrd/G7eWkrrglMH04fn8MtzR9IvM7YXHImEK58/wP0fbOKBjzZjGME1QPdfPp7R+almlxa2FAZEuqCxxcf9H27iv59tw+s3cNqsfPfEgXzvpMG4nJo6EAkXRdVN3PbCivbFwBdPLODemaNIjFOPvQNRGBA5BJvLGvjV3DUs2FQBQH6ai5+fM4IzRuWprbGIyeat2s1PXl1FvdtHcpyd380aw8xxvc0uKyIoDIgcIsMweHdNCb95cx3FNc0ATBuSxb0zR0XlOeci4a7J4+PXc9fywuJCAMb3TeP+y8ZrF9AhUBgQOUzNHj8Pf7yZf32yFY8/gMNm4fqpA7h1+hCSNCQp0iPW7qrj1ueXsaW8EYsFvnfSIG4/dajOGzlECgMiR2h7RSO/fnMtH64vAyA3JY57zh7JuWN7aepApJsYhsGTX2znvrfW4/EHyE2J4++XHMXkwTpj5HAoDIiEyAfrSvnV3LXsrGoC4LiBGdw7cxTD81JMrkwkulQ2tPCjV1a1B/BTR+Twp4vGkZHoNLmyyKUwIBJCbq+fxz7dykMfbabFFwBgRK8UThuRw2kj8xidn6LRApHDtKm0nleWFfHKkiIqGz047VbuOWsE3zq+n/5dHSGFAZFuUFjVxH1vr+Od1SUEOvwLy0uJ59SRwWBw3MAM4uzalihyIFWNHuau3MWcZUWsKqptf3xwThIPXD6eEb008hYKCgMi3ai60cOH68t4b20pn24qp8njb/9eUpydE4dmc9rIXE4elkNqQuydoS6yLx5fgI82lDFnaREfbSjD6w9epuxWCycNy+HCCfmcMiJXBwyFkMKASA9xe/0s3FLJe+tKeX9tKWX1Le3fs1ktHNM/g9NG5nLayFxtiZKYYxgGXxfXMmdpEW+s3EV1k7f9e6N6p3DhhAJmHtVbZwp0E4UBERMEAgarimt5f20p760tZUNpfafvD89L5rSRuZw6Ipcx+anqpS5Rq7TOzWvLi5mztIhNZQ3tj2cnx3HB+HxmTcjXItweoDAgEgZ2Vjbx3rpS3ltbwuLt1fg7LDTITYnjlBHBEYPJgzK1zkAiXrPHz/y1JcxZVsxnm8rb19U47VZOH5nLhRMLmDY4C7t6BfQYhQGRMFPT5OGjDcF1Bp9sKKexwzqDRKeNEzqsM0jXViqJEIZhsHh7NXOWFvHW17upb/G1f29Sv3QunFjAWWN6kerS2hkzKAyIhLEWX+s6g7WlvL+utP3kRAiuM5jUL719nYFOUZRwtLOyiTnLinh1eRGFVc3tj+enubhwQj6zJhTQP0t/d82mMCASIdoWWL2/tpT5a0tZX9J5ncHQ3KT2dQbjCtK0zkBMU+/28tbXu5mztJhF26vaH0902jhrTC8unFjAMf0z9Hc0jCgMiESowqom3l8XXID41baqTusMspPjOHVETus6gyziHVpnIN3LHzD4bHMFry4r4t01Jbi9waZbFgtMGZTFhRPzOWNUHglOndsRjhQGRKJAbZOXjzeWMb91nUFDh/nYBKeNaUOyOG1kHtOH56hlq4RUW1fA15cXd5rGGpSdyIUTC7hgfD69Ul0mVihdoTAgEmVafH6+2lrVvs5gd627/XtWC0zqt6efgeZq5XBUNXp4Y0Uxry4v7tQVMC3BwcxxvZk1oYBxBalqERxBFAZEophhGKzZVcf8tcFGR2t313X6/uCcPesMxvfROgPZv4N1BbxoYj4nD8/R1tcIpTAgEkOKqpt4f20p768r48utlfg6rDPISgquMzh1RC5Th2idgRy4K+Do/GBXwHPHqStgNFAYEIlRtc1ePt5Qxvvryvh4fVmnfd8uR3CdwakjczlleA6Z+mUfUw7WFfDCCQUMy0s2sUIJNYUBEcHjC7BoWxXvrS3hvbWl7OqwzsBigYl99/QzGJidZGKlEmrNHj+byurZUFLPxtJ6VhfX8dW2yvaugHF2K6ePymPWhHx1BYxiCgMi0olhGKzdXde+AHF1ced1BgOzEzltZC6nj8zlqD7p2LTOICJ4/QG2VTS2X/Q3lNSzobSenVVN7OsqoK6AsUVhQEQOaFdNc3s/gy+3VrYvHAPITHRySus6g2lDsnE5tc7AbIGAQWF1056LfmkDG0vq2VrR0On/u44yEp0My01mWF4yQ3KTmDIoSztNYozCgIh0WZ3byycbynl/XSkfri+j3r1nnUG8w8rUwdmcNjKH8X3TibfbcNqtxNmtxDmsOG1WDTGHkGEYlNa1sKG0no2td/kbS+vZVNpAs9e/z9ckxdkZmpvEsLxkhuYmMyw3maF5yVoAKAoDInJ4vP62dQbBUYPimuaDvsZmtQTDgd3aGhRsHT5v/bo1OMQ5bAd+bofvt7123+/T+bV2qyXi9r9XN3raL/Ydh/nrOoSxjpx2K0Nyktov9m0fe6fGR9yfXXqGwoCIHDHDMFhfUt++zqCwqgmPL0CLL9Bp+2I4sFroHBxaw8Ke4GDFabftO2R0CBb7CirBAGLr8D6dn9/2tcO270DS2OJjU1lDpzv99SX1lNe37ONPEgxXA7ISgxf73GSG5SUxNDeZvhkJGoWRQ6IwICLdyucP4PEH2sNBizeAx+/H7W392uff8z1f2/P8tHj3+nrv53qD77u/5+55T/9+58rNYrEQDB8dRjgCAaPTLo69FaS7Ot3pD8tLZmB2opr8SEgoDIhI1AsEjGBw8HYMFvsIDt5vBom9A8re79PxuR1DTYvX3+G5wdd1RXZy3Dfu9IfkJpMUpwN+pPsoDIiI9ID2QLKPEY4Wn5+AAQOyEnWQlJhCYUBERCTGaYWJiIhIjFMYEBERiXEKAyIiIjFOYUBERCTGKQyIiIjEOIUBERGRGKcwICIiEuMUBkRERGKcwoCIiEiMUxgQERGJcQoDIiIiMU5hQEREJMYpDIiIiMQ4hQEREZEYpzAgIiIS4xQGREREYpzCgIiISIxTGBAREYlxCgMiIiIxTmFAREQkxikMiIiIxDiFARERkRinMCAiIhLjFAZERERi3P8DeU7b0Hy2hiAAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "c.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(c.encoder.state_dict(), './encoder_weights')\n",
        "torch.save(c.decoder.state_dict(), './decoder_weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gALFq2jKBP3o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
